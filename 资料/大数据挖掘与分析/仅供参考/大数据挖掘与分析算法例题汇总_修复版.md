# 大数据挖掘与分析算法例题汇总

## 考试重点提示
根据老师消息，考试有以下特点：
- 今年题偏难，包括选择、简答、计算和论述分析题
- PPT上有例子的都要弄明白，保证换个数据能做出来
- 从方法到评价要复习全面
- 课上强调的经典方法需要准确掌握运行流程，能写出流程，能算例题
- 需要掌握经典方法间的优势、不同和关键改进

---

## 第一章 绪论

### 1.1 餐饮行业数据挖掘案例

#### 例题1：餐饮行业完整数据挖掘流程
**题目**：某餐饮连锁企业希望通过数据挖掘提升经营效益，请设计完整的数据挖掘方案。

**数据背景**：
- 内部数据：菜品采购情况，餐饮销售情况，成本单价，会员消费，促销活动
- 外部数据：天气，节假日，竞争对手，周边环境

**解题思路**：

1. **定义目标**：
   - 增加盈利能力→凝聚优质老客户，吸引更多新客户
   - 降低运营成本→减少食材浪费，菜品受欢迎程度预测
   - 扩大经营→选新址建新店

2. **具体目标分解**：
   - 细分餐饮客户，用户画像
   - 面向用户的菜品智能推荐
   - 基于菜品历史销售情况，原材料需求预测
   - 基于整体优化的新店选址

3. **数据采集**：
   - 采集标准：相关性、可靠性、有效性
   - 采集方式：随机抽样、等距抽样、分层抽样、分类抽样、全收集

4. **数据预处理**：
   - 数据缺点：噪音多，不完整，不一致
   - 处理目的：数据一致准确，完整无缺失，维度低方便处理
   - 处理方法：
     * 数据清洗：数据筛选，缺失值处理，数据对齐，属性选择
     * 数据变换：数据标准化，数据转换，数据规约，主成分分析

5. **数据探索**：
   - 考虑因素：
     * 是否达到原来设想的要求
     * 样本中有没有明显的规律和趋势
     * 有没有出现从未设想过的数据状态
     * 属性之间的相关性
   - 分析处理方法：周期性分析，异常值分析，相关分析

6. **问题建模**：
   - 模型类型：分类，聚类，回归拟合，关联规则，时序模型，异常点检测
   - 餐饮行业问题模型：
     * 基于聚类算法的餐饮客户价值分析
     * 基于关联规则算法的动态菜品智能推荐
     * 基于分类/回归预测算法的菜品销量预测
     * 基于整体优化的新店选址

**算法性质说明**：
- 这是一个典型的CRISP-DM过程应用案例
- 涵盖了数据挖掘的完整生命周期
- 体现了业务理解到模型部署的全流程
- 重点在于方法论的应用而非具体算法实现

---

## 第二章 数据准备

### 2.1 数据预处理算法例题

#### 例题2：PCA主成分分析
**题目**：给定以下两个类别的数据点，使用PCA进行降维分析：

**数据集**：
- C1 = {(1,2), (2,3), (3,3), (4,5), (5,5)}
- C2 = {(1,0), (2,1), (3,1), (3,2), (5,3), (6,5)}

**计算过程**：

1. **计算均值向量**：
   - C1均值：μ₁ = (3, 3.6)
   - C2均值：μ₂ = (3.33, 2)

2. **计算协方差矩阵**：
   - 总体均值：μ = (3.17, 2.73)
   - 协方差矩阵：Z = [[2.7636, 2.2545], [2.2545, 3.0182]]

3. **计算特征值和特征向量**：
   - 特征值：λ₁ = 0.6328, λ₂ = 5.1490
   - 对应特征向量：
     * v₁ = (0.7268, -0.6869)ᵀ
     * v₂ = (0.6869, 0.7268)ᵀ

4. **PCA投影方向**：
   - 主成分方向：(0.6869, 0.7268)ᵀ
   - 选择最大特征值对应的特征向量作为投影方向

**结果分析**：
- 第一主成分解释了89%的方差
- 数据投影到一维空间后保留了大部分信息
- 适合用于可视化或后续分类任务

#### 例题3：LDA线性判别分析
**题目**：使用相同数据集，应用LDA进行有监督降维。

**计算过程**：

1. **计算类内散布矩阵S_W**：
   - C1的散布矩阵：S₁ = [[10, 8], [8, 6.8]]
   - C2的散布矩阵：S₂ = [[17.33, 9.33], [9.33, 14]]
   - S_W = S₁ + S₂ = [[27.33, 17.33], [17.33, 20.8]]

2. **计算类间散布矩阵S_B**：
   - 类间差异：μ₁ - μ₂ = (-0.33, 1.6)ᵀ
   - S_B = [μ₁ - μ₂] [μ₁ - μ₂]ᵀ = [[0.11, -0.53], [-0.53, 2.56]]

3. **求解广义特征值问题**：
   - S_W⁻¹S_B的特征值和特征向量
   - LDA投影方向：(0.6656, -0.7463)ᵀ

**结果分析**：
- LDA考虑了类别信息，投影方向更有利于分类
- 与PCA结果不同，体现了有监督学习的优势
- 适用于分类任务的预处理

#### 例题4：数据标准化计算
**题目**：给定一组收入数据，进行不同方式的标准化。

**原始数据**：
- 数据范围：[12,000, 98,000]
- 均值：μ = 54,000
- 标准差：σ = 16,000
- 待标准化值：73,600

**计算过程**：

1. **Z-score标准化**：
   - 公式：z = (x-μ)/σ
   - 计算：z = (73,600-54,000)/16,000 = 1.225

2. **最小-最大标准化（到[0,1]区间）**：
   - 公式：v' = (v-min)/(max-min)
   - 计算：v' = (73,600-12,000)/(98,000-12,000) = 0.716

3. **小数定标标准化**：
   - 找到最大绝对值：98,000
   - 确定j：使max(|v'|)<1的最小整数，j=5
   - 计算：v' = 73,600/10⁵ = 0.736

**结果分析**：
- 不同标准化方法适用于不同场景
- Z-score保持数据的分布形状
- 最小-最大标准化适合有明确边界的数据
- 小数定标保持数据的相对比例关系

---

## 第三章 数据探索

#### 例题5：电商数据统计分析
**题目**：某电商平台销售数据如下，进行基本统计分析。

**数据集字段**：
- 订单ID、用户ID、购买日期、产品类别、产品单价、购买数量、总金额、用户地区

**分析步骤**：

1. **数据概览**：
   - 总订单数：10,000条
   - 总用户数：8,500个
   - 时间范围：2023年1月-12月
   - 产品类别：电子产品、服装、食品、家居、图书

2. **单变量分析**：
   - 订单金额：均值￥280，中位数￥195，标准差￥165
   - 购买数量：均值2.3件，众数1件
   - 用户地区：北京25%，上海20%，广州15%，其他40%

3. **双变量与多变量分析**：
   - 地区与消费水平：北京用户平均消费￥320，上海￥310，其他地区￥260
   - 产品类别与购买频次：电子产品35%，服装25%，食品20%，家居12%，图书8%
   - 季节性分析：第四季度销售额占比40%，第一季度15%

4. **为建模准备**：
   - 创建衍生变量：客单价、购买频率、消费周期
   - 用户分群：高价值用户、普通用户、低价值用户
   - 产品关联性分析：购买电子产品的用户60%会购买配件

**可视化应用**：
- 使用箱线图识别各地区的消费异常值
- 散点图展示购买金额与数量的关系
- 时间序列图展示月度销售趋势

---

## 第四章 距离与相似性

### 4.1 距离度量算法例题

#### 例题6：闵可夫斯基距离计算
**题目**：给定以下4个二维数据点，计算不同类型的闵可夫斯基距离。

**数据点**：
- x₁(1,2), x₂(3,5), x₃(2,0), x₄(4,5)

**计算过程**：

1. **L1距离（曼哈顿距离）**：
   - d(x₁,x₂) = |1-3| + |2-5| = 2 + 3 = 5
   - d(x₁,x₃) = |1-2| + |2-0| = 1 + 2 = 3
   - d(x₁,x₄) = |1-4| + |2-5| = 3 + 3 = 6

2. **L2距离（欧几里得距离）**：
   - d(x₁,x₂) = √[(1-3)² + (2-5)²] = √[4 + 9] = √13 = 3.61
   - d(x₁,x₃) = √[(1-2)² + (2-0)²] = √[1 + 4] = √5 = 2.24
   - d(x₁,x₄) = √[(1-4)² + (2-5)²] = √[9 + 9] = √18 = 4.24

3. **L∞距离（切比雪夫距离）**：
   - d(x₁,x₂) = max(|1-3|, |2-5|) = max(2, 3) = 3
   - d(x₁,x₃) = max(|1-2|, |2-0|) = max(1, 2) = 2
   - d(x₁,x₄) = max(|1-4|, |2-5|) = max(3, 3) = 3

**结果分析**：
- L1距离对坐标轴方向的变化敏感
- L2距离是最常用的距离度量
- L∞距离关注最大差异维度
- 不同距离度量适用于不同应用场景

#### 例题7：二元属性相似性计算
**题目**：给定3个人的属性信息，计算Jaccard相似系数。

**数据**：
- Jack: {M, Y, N, P, N, N, N}
- Mary: {F, Y, N, P, N, P, N}
- Jim: {M, Y, P, N, N, N, N}

**属性含义**：性别、是否有车、是否有房、是否有存款、是否有贷款、是否有投资、是否有保险

**计算过程**：

1. **Jack与Jim的相似性**：
   - 相同属性：M, Y, N, N, N, N (6个)
   - 不同属性：P vs N (1个)
   - Jaccard系数 = 6/7 = 0.857

2. **Jack与Mary的相似性**：
   - 相同属性：Y, N, P, N, N (5个)
   - 不同属性：M vs F, N vs P (2个)
   - Jaccard系数 = 5/7 = 0.714

3. **Jim与Mary的相似性**：
   - 相同属性：Y, N, N, N (4个)
   - 不同属性：M vs F, P vs N, N vs P (3个)
   - Jaccard系数 = 4/7 = 0.571

**结果分析**：
- Jack和Jim最相似（0.857）
- Jim和Mary最不相似（0.571）
- Jaccard系数适合处理二元属性数据
- 忽略了共同缺失的属性

#### 例题8：文本余弦相似度计算
**题目**：给定两个文档的TF-IDF向量，计算余弦相似度。

**文档向量**：
- 文档A：(0.2, 0.5, 0.1, 0.3, 0.0)
- 文档B：(0.1, 0.3, 0.4, 0.2, 0.6)

**计算过程**：

1. **计算点积**：
   - A·B = 0.2×0.1 + 0.5×0.3 + 0.1×0.4 + 0.3×0.2 + 0.0×0.6
   - A·B = 0.02 + 0.15 + 0.04 + 0.06 + 0 = 0.27

2. **计算向量长度**：
   - ||A|| = √(0.2² + 0.5² + 0.1² + 0.3² + 0.0²) = √(0.04 + 0.25 + 0.01 + 0.09 + 0) = √0.39 = 0.625
   - ||B|| = √(0.1² + 0.3² + 0.4² + 0.2² + 0.6²) = √(0.01 + 0.09 + 0.16 + 0.04 + 0.36) = √0.66 = 0.812

3. **计算余弦相似度**：
   - cos(A,B) = (A·B) / (||A|| × ||B||) = 0.27 / (0.625 × 0.812) = 0.27 / 0.508 = 0.531

**结果分析**：
- 相似度0.531表示中等相似程度
- 余弦相似度忽略向量长度差异
- 适用于文本挖掘和推荐系统
- 值域为[0,1]，1表示完全相似

#### 例题9：动态时间规整(DTW)
**题目**：给定两个时间序列，使用DTW计算它们之间的距离。

**序列数据**：
- A = [2, 4, 6, 8, 10, 12, 14, 16]
- B = [1, 7, 9, 3, 5, 8, 9, 3, 4]

**计算过程**：

1. **构建距离矩阵**：
   - 计算每对元素之间的绝对距离
   - 例如：d(2,1) = 1, d(2,7) = 5, d(4,1) = 3, d(4,7) = 3

2. **累计距离矩阵计算**：
   - DTW(i,j) = d(i,j) + min{DTW(i-1,j), DTW(i,j-1), DTW(i-1,j-1)}
   - 逐步填充整个矩阵

3. **寻找最优路径**：
   - 从右下角回溯到左上角
   - 选择累计距离最小的路径

4. **最终DTW距离**：
   - 路径上的累计距离和
   - 考虑了时间轴的弹性变形

**结果分析**：
- DTW能够处理不同长度的序列
- 允许时间轴的非线性对齐
- 适用于语音识别、手势识别等
- 计算复杂度较高，但效果优于欧几里得距离

---

## 第五章 频繁模式与关联规则

### 5.1 Apriori算法例题

#### 例题10：Apriori算法完整计算
**题目**：给定以下事务数据集，使用Apriori算法挖掘频繁项集（最小支持度=3）。

**事务数据集**：
```
TID Items
1   {Bread, Milk}
2   {Beer, Bread, Diaper, Eggs}
3   {Beer, Coke, Diaper, Milk}
4   {Beer, Bread, Diaper, Milk}
5   {Bread, Coke, Diaper, Milk}
```

**计算过程**：

1. **第一次扫描 - 寻找频繁1项集**：
   - Bread: 4次
   - Milk: 4次
   - Beer: 3次
   - Diaper: 4次
   - Coke: 2次
   - Eggs: 1次
   
   **频繁1项集L₁**：{Bread:4, Milk:4, Beer:3, Diaper:4}

2. **候选2项集生成C₂**：
   - {Bread, Milk}, {Bread, Beer}, {Bread, Diaper}
   - {Milk, Beer}, {Milk, Diaper}
   - {Beer, Diaper}

3. **第二次扫描 - 计算支持度**：
   - {Bread, Milk}: 3次 (T1, T4, T5)
   - {Bread, Beer}: 2次 (T2, T4)
   - {Bread, Diaper}: 3次 (T2, T4, T5)
   - {Milk, Beer}: 2次 (T3, T4)
   - {Milk, Diaper}: 3次 (T3, T4, T5)
   - {Beer, Diaper}: 3次 (T2, T3, T4)

   **频繁2项集L₂**：{Bread,Milk:3, Bread,Diaper:3, Milk,Diaper:3, Beer,Diaper:3}

4. **候选3项集生成C₃**：
   - {Bread, Milk, Diaper}
   - {Beer, Milk, Diaper}

5. **第三次扫描 - 计算支持度**：
   - {Bread, Milk, Diaper}: 2次 (T4, T5)
   - {Beer, Milk, Diaper}: 2次 (T3, T4)

   **频繁3项集L₃**：空（都不满足最小支持度3）

**最终频繁项集**：
- L₁: {Bread:4, Milk:4, Beer:3, Diaper:4}
- L₂: {Bread,Milk:3, Bread,Diaper:3, Milk,Diaper:3, Beer,Diaper:3}

#### 例题11：关联规则生成与评价
**题目**：基于例10的频繁项集，生成关联规则并计算置信度和提升度。

**规则生成过程**：

1. **从频繁2项集生成规则**：
   - {Bread, Milk} → Bread: 置信度 = 3/3 = 100%
   - {Bread, Milk} → Milk: 置信度 = 3/4 = 75%
   - {Bread, Diaper} → Bread: 置信度 = 3/3 = 100%
   - {Bread, Diaper} → Diaper: 置信度 = 3/4 = 75%
   - {Milk, Diaper} → Milk: 置信度 = 3/4 = 75%
   - {Milk, Diaper} → Diaper: 置信度 = 3/4 = 75%
   - {Beer, Diaper} → Beer: 置信度 = 3/3 = 100%
   - {Beer, Diaper} → Diaper: 置信度 = 3/4 = 75%

2. **计算提升度（假设最小置信度=70%）**：
   - Lift(Bread → Milk) = P(Bread∩Milk)/[P(Bread)×P(Milk)] = (3/5)/[(4/5)×(4/5)] = 0.6/0.64 = 0.9375
   - Lift(Milk → Bread) = P(Milk∩Bread)/[P(Milk)×P(Bread)] = 0.9375
   - Lift(Diaper → Beer) = P(Diaper∩Beer)/[P(Diaper)×P(Beer)] = (3/5)/[(4/5)×(3/5)] = 0.6/0.36 = 1.67

**结果分析**：
- 提升度>1表示正相关，如Diaper → Beer
- 提升度<1表示负相关，如Bread → Milk
- 提升度=1表示无相关性
- 强规则：{Beer, Diaper} → Beer (置信度100%，提升度1.67)

### 5.2 FP-Growth算法例题

#### 例题12：FP-Growth算法完整演示
**题目**：给定以下事务数据集，使用FP-Growth算法挖掘频繁项集（最小支持度=2）。

**数据集**：
```
TID Items (ordered frequent items)
100 {f, c, a, m, p}
200 {f, c, a, b, m}
300 {f, b}
400 {c, b, p}
500 {f, c, a, m, p}
```

**计算过程**：

1. **第一次扫描 - 计算项频次**：
   - f: 4次, c: 4次, a: 3次, b: 3次, m: 3次, p: 2次

2. **按支持度降序排列**：f, c, a, b, m, p

3. **构建FP-Tree**：
   - 根节点：null
   - 插入事务100: null→f→c→a→m→p
   - 插入事务200: null→f→c→a→b→m
   - 插入事务300: null→f→b
   - 插入事务400: null→c→b→p
   - 插入事务500: null→f→c→a→m→p

4. **项头表**：
   - f: [f(4)]
   - c: [c(4)]
   - a: [a(3)]
   - b: [b(3)]
   - m: [m(3)]
   - p: [p(2)]

5. **从项头表底部开始挖掘**：
   
   **对于项p**：
   - 条件模式基：{f,c,a,m:2}, {c,b:1}
   - 条件FP-Tree：{f,c,a,m}
   - 频繁模式：{p}, {f,p}, {c,p}, {a,p}, {m,p}, {f,c,p}, {f,a,p}, {c,a,p}, {f,m,p}, {c,m,p}, {a,m,p}, {f,c,a,p}, {f,c,m,p}, {f,a,m,p}, {c,a,m,p}, {f,c,a,m,p}

   **对于项m**：
   - 条件模式基：{f,c,a:2}, {f,c,a:1}, {f,c,a:1}
   - 条件FP-Tree：{f,c,a}
   - 频繁模式：{m}, {f,m}, {c,m}, {a,m}, {f,c,m}, {f,a,m}, {c,a,m}, {f,c,a,m}

   **对于项b**：
   - 条件模式基：{f,c,a:1}, {f:1}, {c:1}
   - 条件FP-Tree：{f}, {c}
   - 频繁模式：{b}, {f,b}, {c,b}

   **对于项a**：
   - 条件模式基：{f,c:3}
   - 条件FP-Tree：{f,c}
   - 频繁模式：{a}, {f,a}, {c,a}, {f,c,a}

   **对于项c**：
   - 条件模式基：{f:4}
   - 条件FP-Tree：{f}
   - 频繁模式：{c}, {f,c}

**最终频繁项集**：
- 包含所有支持度≥2的项集及其组合

---

## 第六章 聚类算法

### 6.1 K-means算法例题

#### 例题13：K-means算法完整演示
**题目**：给定以下8个二维数据点，使用K-means算法进行聚类（k=3）。

**数据点**：
- A(2,2), B(3,2), C(3,3), D(5,3), E(6,3), F(7,3), G(7,4), H(8,5)

**算法过程**：

**第1次迭代**：
1. **初始中心点**：A(2,2), D(5,3), G(7,4)
2. **分配阶段**：
   - 计算各点到中心点的距离：
     * A到A:0, A到D:3.16, A到G:7.07 → 分配到簇1
     * B到A:1, B到D:2.24, B到G:6.08 → 分配到簇1
     * C到A:1.41, C到D:2, C到G:5.66 → 分配到簇1
     * D到A:3.16, D到D:0, D到G:2.83 → 分配到簇2
     * E到A:4.12, E到D:1, E到G:1.41 → 分配到簇2
     * F到A:5.10, F到D:2, F到G:1 → 分配到簇3
     * G到A:7.07, G到D:2.83, G到G:0 → 分配到簇3
     * H到A:8.49, H到D:4.24, H到G:1.41 → 分配到簇3

3. **更新阶段**：
   - 簇1: {A,B,C} → 新中心 (2.67, 2.33)
   - 簇2: {D,E} → 新中心 (5.5, 3)
   - 簇3: {F,G,H} → 新中心 (7.33, 4)

**第2次迭代**：
1. **分配阶段**：
   - 重新计算距离并分配点
   - 结果：簇1{A,B,C}, 簇2{D,E}, 簇3{F,G,H}

2. **更新阶段**：
   - 中心点不再变化，算法收敛

**最终聚类结果**：
- 簇1: {A(2,2), B(3,2), C(3,3)}
- 簇2: {D(5,3), E(6,3)}
- 簇3: {F(7,3), G(7,4), H(8,5)}

**算法性质分析**：
- 收敛速度快，2次迭代即收敛
- 产生了3个紧凑的球形簇
- 对初始中心点选择敏感
- 适合发现球形分布的数据

### 6.2 层次聚类例题

#### 例题14：单链接层次聚类
**题目**：给定6个城市的距离矩阵，使用单链接方法进行层次聚类。

**距离矩阵**：
```
     BA  FI  MI/TO  NA  RM
BA   0   843  1098   294  1448
FI  843   0   659    714  875
MI/TO 1098 659   0    1380  1424
NA   294  714  1380    0   1235
RM  1448 875  1424   1235   0
```

**凝聚过程**：

1. **第1步**：合并最接近的城市对
   - 最小距离：BA-NA = 294
   - 新簇：{BA,NA}

2. **第2步**：更新距离矩阵
   - 使用单链接：d({BA,NA},x) = min(d(BA,x), d(NA,x))
   - 新距离矩阵：
     ```
          {BA,NA}  FI  MI/TO  RM
     {BA,NA}   0    714  1098  1235
     FI       714    0    659   875
     MI/TO   1098   659    0   1424
     RM      1235   875   1424    0
     ```

3. **第3步**：继续合并
   - 最小距离：FI-MI/TO = 659
   - 新簇：{FI,MI/TO}

4. **第4步**：继续凝聚直到只有一个簇

**树状图构建**：
- 层次结构展示了聚类过程
- 可以根据需要选择不同的聚类数量
- 单链接容易产生链式效应

### 6.3 DBSCAN算法例题

#### 例题15：DBSCAN算法参数影响分析
**题目**：给定以下数据点，分析不同参数对DBSCAN聚类结果的影响。

**数据点**：
- A(1,1), B(1,2), C(2,1), D(2,2), E(3,3), F(8,8), G(8,9), H(9,8), I(9,9)

**参数设置1**：MinPts = 4, Eps = 1.5
- 核心点：A,B,C,D（邻域内都包含≥4个点）
- 边界点：E
- 噪声点：F,G,H,I
- 结果：1个簇{A,B,C,D,E} + 4个噪声点

**参数设置2**：MinPts = 4, Eps = 2.0
- 核心点：A,B,C,D,E,F,G,H,I
- 边界点：无
- 噪声点：无
- 结果：2个簇{A,B,C,D,E}和{F,G,H,I}

**参数设置3**：MinPts = 2, Eps = 1.5
- 核心点：所有点
- 边界点：无
- 噪声点：无
- 结果：2个簇{A,B,C,D,E}和{F,G,H,I}

**结果分析**：
- Eps值越大，簇的范围越大
- MinPts值越小，核心点越多
- 参数选择对结果影响显著
- 需要根据数据特点调整参数

### 6.4 谱聚类例题

#### 例题16：谱聚类算法演示
**题目**：给定以下相似度矩阵，使用谱聚类进行聚类（k=2）。

**相似度矩阵**：
```
      A   B   C   D   E
A  1.0 0.8 0.1 0.1 0.2
B  0.8 1.0 0.1 0.1 0.2
C  0.1 0.1 1.0 0.9 0.7
D  0.1 0.1 0.9 1.0 0.7
E  0.2 0.2 0.7 0.7 1.0
```

**算法过程**：

1. **构建度矩阵D**：
   ```
   D = diag(1.2, 1.2, 2.8, 2.8, 2.8)
   ```

2. **计算拉普拉斯矩阵L = D - W**：
   ```
   L = [[ 0.2, -0.8, -0.1, -0.1, -0.2],
        [-0.8,  0.2, -0.1, -0.1, -0.2],
        [-0.1, -0.1,  1.8, -0.9, -0.7],
        [-0.1, -0.1, -0.9,  1.8, -0.7],
        [-0.2, -0.2, -0.7, -0.7,  1.8]]
   ```

3. **计算特征值和特征向量**：
   - 前k=2个最小特征值对应的特征向量
   - 组成n×k矩阵U

4. **对U的行进行K-means聚类**：
   - 特征空间中的点更容易分离
   - 最终聚类结果：{A,B}和{C,D,E}

**结果分析**：
- 谱聚类能发现非球形簇
- 基于图论的分割方法
- 适用于社交网络、图像分割等
- 计算复杂度较高，适合中小数据集

---

## 第七章 离群点检测

### 7.1 基于概率的方法例题

#### 例题17：一元离群点检测
**题目**：某城市10年7月平均温度如下，检测离群点（假设正态分布，α=0.05）。

**温度数据**：
{24.0, 28.9, 28.9, 29.0, 29.1, 29.1, 29.2, 29.2, 29.3, 29.4}

**计算过程**：

1. **计算统计量**：
   - 均值：μ = 28.5
   - 标准差：σ = 1.8
   - 样本数：n = 10

2. **正态分布假设检验**：
   - H₀：数据服从正态分布N(μ,σ²)
   - 显著性水平：α = 0.05
   - 临界值：z(α/2) = 1.96

3. **检测离群点**：
   - 计算每个点的Z分数：
     * Z(24.0) = (24.0-28.5)/1.8 = -2.5
     * Z(28.9) = (28.9-28.5)/1.8 = 0.22
     * ...
     * Z(29.4) = (29.4-28.5)/1.8 = 0.5

4. **判断标准**：
   - |Z| > 1.96 时为离群点
   - 只有24.0满足条件

**结果分析**：
- 24.0被识别为离群点
- 可能表示异常气候事件
- 需要进一步调查原因
- 方法简单但假设较强

### 7.2 LOF算法例题

#### 例题18：局部离群因子计算
**题目**：给定以下6个数据点，计算点P的LOF值（k=3）。

**数据点**：
- P(5,5), A(4,4), B(5,4), C(6,4), D(4,6), E(6,6), F(8,8)

**计算过程**：

1. **计算k-距离**：
   - P到各点的距离：
     * d(P,A) = 1.41, d(P,B) = 1, d(P,C) = 1.41
     * d(P,D) = 1.41, d(P,E) = 1.41, d(P,F) = 4.24
   - k-distance(P) = 第3近邻距离 = 1.41

2. **k-距离邻域N_k(P)**：
   - N₃(P) = {A, B, C, D, E}

3. **计算可达距离**：
   - reach-dist₃(P,A) = max{k-dist(P), d(P,A)} = max{1.41, 1.41} = 1.41
   - reach-dist₃(P,B) = max{1.41, 1.0} = 1.41
   - reach-dist₃(P,C) = max{1.41, 1.41} = 1.41
   - reach-dist₃(P,D) = max{1.41, 1.41} = 1.41
   - reach-dist₃(P,E) = max{1.41, 1.41} = 1.41

4. **计算局部可达密度**：
   - lrd₃(P) = |N₃(P)| / Σ reach-dist₃(P,o)
   - lrd₃(P) = 5 / (1.41+1.41+1.41+1.41+1.41) = 5 / 7.05 = 0.71

5. **计算LOF值**：
   - 对每个邻居o∈N₃(P)，计算lrd₃(o)
   - LOF₃(P) = Σ[lrd₃(o)/lrd₃(P)] / |N₃(P)|
   - 假设计算结果：LOF₃(P) = 1.2

**结果分析**：
- LOF ≈ 1.2 > 1，P可能是局部离群点
- 考虑了局部密度变化
- 适用于密度不均匀的数据
- 对参数k敏感

### 7.3 基于聚类的离群点检测

#### 例题19：CBLOF算法应用
**题目**：使用聚类结果检测离群点。

**聚类结果**：
- 大簇C₁：{100个点，中心(5,5)，半径2}
- 小簇C₂：{10个点，中心(10,10)，半径1}
- 小簇C₃：{5个点，中心(15,15)，半径0.5}

**CBLOF计算**：

1. **按簇大小排序**：
   - C₁(100), C₂(10), C₃(5)

2. **确定阈值**：
   - 大簇阈值：size(C₁) = 100
   - 小簇：C₂, C₃

3. **计算CBLOF值**：
   - 对于大簇中的点：CBLOF = size(C₁) × distance(point, center(C₁))
   - 对于小簇中的点：CBLOF = size(C₁) × distance(point, center(C₁)) + size(C₂) × distance(point, center(C₂))

4. **检测离群点**：
   - CBLOF值低的点为离群点
   - 通常是小簇中远离大簇中心的点

**结果分析**：
- 考虑了簇的大小和距离
- 能检测全局和局部离群点
- 适用于聚类后的离群点检测
- 依赖于聚类质量

---

## Web挖掘 - PageRank算法

### 8.1 基本PageRank例题

#### 例题20：简单PageRank计算
**题目**：给定以下网页链接结构，计算各页面的PageRank值。

**网页结构**：
- 页面：y, a, m
- 链接关系：
  - y → {y, a}
  - a → {y, m}
  - m → {a, m}

**计算过程**：

1. **建立方程组**：
   - ry = ry/2 + ra/2
   - ra = ry/2 + rm/2
   - rm = ra/2 + rm/2

2. **整理方程组**：
   - ry - ry/2 - ra/2 = 0 → ry/2 - ra/2 = 0 → ry = ra
   - ra - ry/2 - rm/2 = 0
   - rm - ra/2 - rm/2 = 0 → rm/2 - ra/2 = 0 → rm = ra

3. **求解**：
   - 从ry = ra和rm = ra，得：ry = ra = rm
   - 归一化：ry + ra + rm = 1
   - 解得：ry = ra = rm = 1/3

**结果分析**：
- 三个页面PageRank值相等
- 每个页面都有相同的入链数量
- 体现了PageRank的平衡特性

### 8.2 带随机游走的PageRank

#### 例题21：Google矩阵计算
**题目**：使用随机游走模型重新计算例20的PageRank（β=0.8）。

**计算过程**：

1. **构建转移矩阵M**：
   ```
   M = [[0.5, 0.5, 0.0],
        [0.5, 0.0, 0.5],
        [0.0, 0.5, 0.5]]
   ```

2. **构建Google矩阵A**：
   - A = βM + (1-β)/N × J
   - A = 0.8M + 0.2/3 × J
   ```
   A = [[0.4, 0.4, 0.067],
        [0.4, 0.067, 0.4],
        [0.067, 0.4, 0.4]]
   ```

3. **幂迭代法求解**：
   - 初始：r⁽⁰⁾ = [1/3, 1/3, 1/3]ᵀ
   - 第1次迭代：r⁽¹⁾ = A × r⁽⁰⁾ = [0.289, 0.289, 0.289]ᵀ
   - 第2次迭代：r⁽²⁾ = A × r⁽¹⁾ = [0.289, 0.289, 0.289]ᵀ
   - 收敛：r = [0.289, 0.289, 0.289]ᵀ

4. **归一化**：
   - ry = ra = rm ≈ 0.333

**结果分析**：
- 随机游走确保了收敛性
- 解决了死端和蜘蛛陷阱问题
- β值影响收敛速度和结果稳定性
- 实际Web图中效果更明显

### 8.3 主题特定PageRank

#### 例题22：个性化PageRank计算
**题目**：针对体育主题计算个性化PageRank。

**设置**：
- 主题集合S = {体育相关页面}
- β = 0.8
- 假设页面a属于体育主题

**修改的Google矩阵**：
```
A[i,j] = {
  0.8 × M[i,j] + 0.2/|S|, if j ∈ S
  0.8 × M[i,j],           otherwise
}
```

**计算特点**：
- 体育页面获得额外的随机传送概率
- 非体育页面的PageRank值相对降低
- 体现了主题偏向性
- 适用于个性化搜索和推荐

**应用场景**：
- 个性化搜索引擎
- 主题相关的网页排名
- 垂直搜索领域
- 社交网络影响力分析

---

## 算法综合应用例题

### 例题23：完整数据挖掘流程应用
**题目**：某电商平台希望识别高价值客户并预测其购买行为，设计完整的数据挖掘方案。

**解题思路**：

1. **业务理解**：
   - 目标：提高客户留存率和客单价
   - 关键指标：购买频率、客单价、客户生命周期价值

2. **数据准备**：
   - 收集数据：用户基本信息、购买历史、浏览行为、评价记录
   - 数据清洗：处理缺失值、异常值、重复记录
   - 特征工程：
     * RFM特征：最近购买时间、购买频率、消费金额
     * 行为特征：浏览时长、页面访问深度、收藏夹使用
     * 社交特征：好友数量、分享行为、评价活跃度

3. **数据探索**：
   - 描述统计：各特征的基本统计量
   - 相关性分析：特征间关系探索
   - 可视化：客户分布、消费模式可视化

4. **建模过程**：
   
   **客户分群（聚类）**：
   - 使用K-means算法对客户进行分群
   - 确定最优K值（肘部法则）
   - 解释各簇特征：高价值客户、潜力客户、流失风险客户
   
   **购买预测（分类）**：
   - 构建分类模型预测客户是否会购买
   - 特征选择：使用信息增益或卡方检验
   - 模型比较：决策树、逻辑回归、随机森林
   
   **关联规则挖掘**：
   - 发现商品间的关联关系
   - 使用Apriori算法挖掘频繁项集
   - 生成推荐规则

5. **模型评估**：
   - 聚类评估：轮廓系数、Calinski-Harabasz指数
   - 分类评估：准确率、精确率、召回率、F1分数
   - 关联规则评估：支持度、置信度、提升度

6. **部署应用**：
   - 实时推荐系统
   - 客户生命周期管理
   - 精准营销活动

**算法选择理由**：
- K-means：简单高效，适合大规模客户分群
- 随机森林：处理非线性关系，特征重要性评估
- Apriori：经典关联规则算法，易于理解和实现

**预期效果**：
- 客户留存率提升15-20%
- 推荐点击率提升25-30%
- 营销成本降低10-15%

---

## 考试重点算法总结

### 必须掌握的算法
1. **Apriori算法**：候选集生成、剪枝策略、支持度计算
2. **FP-Growth算法**：FP-Tree构建、条件模式基、递归挖掘
3. **K-means算法**：中心点初始化、分配更新过程、收敛判断
4. **层次聚类**：单链接、全链接、平均链接的距离计算
5. **DBSCAN算法**：核心点识别、簇扩展、参数影响
6. **PageRank算法**：矩阵表示、幂迭代法、随机游走

### 重点计算题型
1. **距离度量**：闵可夫斯基距离、马哈拉诺比斯距离、余弦相似度
2. **关联规则**：支持度、置信度、提升度计算
3. **数据预处理**：标准化、归一化、主成分分析
4. **聚类评估**：轮廓系数、簇内平方和、纯度计算
5. **离群点检测**：Z分数、LOF值、CBLOF计算

### 算法比较要点
1. **Apriori vs FP-Growth**：候选集生成 vs 无候选生成
2. **K-means vs 层次聚类**：划分 vs 层次、球形 vs 任意形状
3. **DBSCAN vs K-means**：密度 vs 距离、任意形状 vs 球形
4. **各种距离度量**：适用场景、计算复杂度、优缺点

---

*本文档包含了大数据挖掘与分析课程的所有重要算法例题，每个例题都提供了详细的计算过程、结果分析和算法性质说明，适合考试复习和算法理解。*