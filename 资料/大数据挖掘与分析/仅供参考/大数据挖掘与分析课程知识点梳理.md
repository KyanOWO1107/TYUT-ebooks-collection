# 大数据挖掘与分析课程知识点梳理

## 考试重点提示
根据老师消息，考试有以下特点：
- 今年题偏难，包括选择、简答、计算和论述分析题
- 推荐算法章节不在考试范围，其他章节都有涉及
- PPT上有例子的都要弄明白，保证换个数据能做出来
- 从方法到评价要复习全面
- 课上强调的经典方法需要准确掌握运行流程，能写出流程，能算例题
- 需要掌握经典方法间的优势、不同和关键改进
- 不常用方法需掌握基本思想
- 针对具体案例能完整应用大数据挖掘思路进行阐述
- 闭频繁和极大频繁需掌握，具体到近似频繁模式了解即可
- PCA需要理解，知道干什么用的，做什么，原理是什么

---

## 第一章 绪论

### 1.1 数据挖掘定义与基本概念
- **数据挖掘定义**：从大量的数据中挖掘那些令人感兴趣的、有用的、隐含的、先前未知的和可能有用的模式或知识
- **数据分析定义**：指用适当的统计分析方法对收集来的大量数据进行分析，将它们加以汇总和理解并消化，以求最大化地开发数据的功能，发挥数据的作用
- **数据挖掘与分析目的**：揭示事实，发现趋势，预测未来

### 1.2 数据挖掘应用领域
- 公共安全：犯罪分析
- 医疗保健领域
- 城市规划
- 位置信息分析
- 情感分析
- 社交网络分析
- 精准销售
- 海平面温度预测

### 1.3 数据挖掘过程
#### 1.3.1 CRISP-DM过程模型
- 业务理解
- 数据理解
- 数据准备
- 建模
- 评估
- 部署

#### 1.3.2 本书介绍的数据挖掘过程
1. **挖掘目标的定义**
   - 需要解决问题的明确定义
   - 对有关数据的了解
   - 数据挖掘结果对业务作用效力的预测
   - 从问题定义和效果评估两个角度定义目标

2. **数据的准备**
   - 数据的选择：从数据源中搜索所有与业务对象有关的数据
   - 数据的质量分析：评估数据质量
   - 数据的预处理：数据清洗、数据集成、数据归约和数据变换

3. **数据的探索**
   - 描述统计：均值、频率、众数、百分位数、中位数、极差、方差等
   - 数据的可视化：频次图、散点图、箱体图等
   - 数据探索的建模活动

4. **模型的建立**
   - 模型结构：分类、聚类、回归拟合、关联规则、时序模型、异常点检测等
   - 模型操作流程

5. **模型的评估**
   - 模式兴趣度度量：客观度量(支持度、置信度等)和主观度量
   - 评估指标：精确度、LIFT、ROC、Gain图等

6. **模型的部署**
   - 提供给分析人员做参考
   - 开发并部署到实际的业务系统中

### 1.4 数据挖掘任务及过程
- 挖掘频繁模式、关联和相关性
- 分类
- 聚类
- 离群点检测
- 挖掘模式

---

## 第二章 数据准备

### 2.1 认识数据

#### 2.1.1 数据定义与特征
- **数据定义**："Data are pieces of information that represent the qualitative or quantitative attributes of a variable or set of variables"
- **数据矩阵**：通常数据可以抽象成一个n×d的数据矩阵
  - 行：实体、实例、样本、记录、事务、对象、数据点、特征向量和元组等
  - 列：属性、性质、特征、维度、域等

#### 2.1.2 属性类型
- **数值型(Numeric)**
  - 区间标度：只有差值有意义(如温度)
  - 比例标度：差值和比例都有意义(如年龄)
- **类别型(Categorical)**
  - 名义类(Nominal)：只有相同有意义，包括二元和多元
  - 序数类(Ordinal)：有顺序关系(如教育水平)

#### 2.1.3 数据类型
- **记录数据**：关系记录、文本数据、事务数据
- **图形和网络数据**：万维网、社交网络
- **有序数据**：时间数据、序列数据
- **空间、图像和多媒体数据**：空间数据、图像数据、视频数据

#### 2.1.4 数据的几何和代数描述
- 数据集中每一行可以看作d维空间的点或d维空间列向量
- 数据矩阵平均：$\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$
- 中心数据矩阵：$X_c = [\bar{x}, x_2-\bar{x}, ..., x_n-\bar{x}]$
- 点乘：$x \cdot y = \sum_{i=1}^{d}x_i y_i$
- 欧几里得范数：$||x|| = \sqrt{x \cdot x}$
- 两向量间的距离：$d(x,y) = ||x-y||$
- 两向量间的夹角：$\cos\theta = \frac{x \cdot y}{||x|| \cdot ||y||}$

### 2.2 数据质量分析

#### 2.2.1 数据质量问题
- **不完整**：缺失值(如职业="")
- **噪声**：数据错误(如工资="-100")
- **不一致**：编码不一致(如年龄="42" VS 生日="08/08/1982")

#### 2.2.2 数据质量分析指标
- 总记录数：表征数据规模
- 唯一值数：表征数据多样性
- 空值占比：表征无效数据的影响程度
- 非零占比：表征非零值的影响程度
- 正数占比：表征正值的影响程度
- 负数占比：表征负值的影响程度

### 2.3 数据预处理

#### 2.3.1 数据清洗
- **缺失数据处理**
  - 忽视：较小缺失率
  - 去掉有缺失值的样本或属性
  - 人工补全缺失值
  - 自动补全缺失值：固定值、均值、基于算法
  
- **插补法**
  - 均值插补：用属性存在值的平均值或众数来插补
  - 回归插补
  - 极大似然估计

- **噪声数据处理**
  - 回归法：用函数拟合数据来光滑数据
  - 均值平滑法：用临近若干数据的均值替换原始数据
  - 离群点分析：通过聚类检测离群点并删除
  - 局部离群因子(LOF)

#### 2.3.2 数据集成
- **实体识别**：匹配来自不同数据源的现实世界的实体
- **属性集成**：对同一实体的不同属性值进行整合
- **冗余性识别**：检测和处理冗余数据

#### 2.3.3 数据规约
- **维归约**
  - 特征选择：顺序选取法、顺序筛除法、优化算法
  - 特征提取：主成分分析(PCA)、线性判别分析(LDA)
  
- **数量规约**
  - 参数估计
  - 非参数方法
  - 聚类方法
  - 采样

- **数据压缩**
  - 有损压缩
  - 无损压缩

#### 2.3.4 数据变换
- **标准化**
  - 最小-最大标准化：$v' = \frac{v-min}{max-min} \times (new\_max-new\_min) + new\_min$
  - Z-score标准化：$v' = \frac{v-\mu}{\sigma}$

- **离散化**
  - 等宽离散化
  - 等频离散化
  - 基于聚类的离散化

- **语义转换**
  - 将字符型属性值转换为整型数据

---

## 第三章 数据探索

### 3.1 衍生变量
- **定义**：由其它既有变量通过不同形式的组合而衍生出的变量
- **原则**：
  - 衍生变量能够客观反映事物的特征
  - 衍生变量与数据挖掘的业务目标有一定的联系
- **方法**：
  - 对多个列变量进行组合
  - 按照维度分类汇总
  - 对某个变量进一步分解
  - 对具有时间序列特征的变量提取时序特征

### 3.2 数据的统计

#### 3.2.1 基本描述统计
- **表示位置的统计量**
  - 算术平均值：$\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$
  - 中位数：将数据由小到大排序后位于中间位置的数值
  - 众数：出现频率最高的值

- **表示数据散度的统计量**
  - 标准差：$s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2}$
  - 方差：$s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2$
  - 极差：$range = max(x) - min(x)$

- **表示分布形状的统计量**
  - 偏度：反映分布的对称性
  - 峰度：用作衡量偏离正态分布的尺度

#### 3.2.2 多元汇总统计
- **多元数据的位置度量**：$\bar{X} = (\bar{x}_1, \cdots, \bar{x}_d)$
- **多元数据的散布**
  - 协方差矩阵：$s_{ij} = cov(x_i, x_j)$
  - 相关矩阵：$r_{ij} = \frac{cov(x_i \cdot x_j)}{s_i s_j}$

### 3.3 数据可视化

#### 3.3.1 基本统计描述的图表示
- **正态分布曲线**：
  - 从μ-σ到μ+σ，包含68%的测量值
  - 从μ-2σ到μ+2σ，包含95%的测量值
  - 从μ-3σ到μ+3σ，包含99.7%的测量值

- **箱线图**：5数概括(最小值、Q1、中位数、Q3、最大值)
  - 四分位：Q1(25%)、Q3(75%)
  - 四分位极差：Q3-Q1
  - 离群点：大于/小于1.5×IQR的值

- **分位数图**：展示整体行为和异常行为
- **Q-Q图**：展示从一个分布到另一个分布的漂移
- **直方图**：比箱线图信息更丰富
- **散点图**：可以看成聚类或离群点的初探索，用于相关性分析和分类判断

#### 3.3.2 高级可视化技术
- **基于像素的可视化**：对m维数据创建m个窗口
- **几何投影的可视化**：
  - 直接可视化
  - 地形图
  - 投影追寻技术
  - 剖视图
  - 超切片
  - 平行坐标
- **等值线图**
- **平行坐标系**：k个平行的坐标表示属性
- **切尔诺夫脸**：用卡通脸表示数据特征
- **人物线条画**
- **树图**：把层次数据显示成嵌套矩形的集合

---

## 第四章 距离与相似性

### 4.1 基本概念
- **邻近性**：相似性和相异性统称为邻近性
- **属性类型**：标称属性、序数属性、数值属性
- **数据结构**：
  - 数据矩阵：存放数据对象
  - 相异性矩阵：存放数据对象的相异性值

### 4.2 不同类型数据的邻近性度量

#### 4.2.1 二元属性的临近性
- **对称的二元相异性**：所有状态同等重要
- **非对称的二元相异性**：非对称性，使用Jaccard系数

#### 4.2.2 标称属性的邻近性
- 直接度量
- 转化成二元属性：$d(i,j) = \frac{p-m}{p}$

#### 4.2.3 数值属性的相似性
- **数值数据的标准化**
  - Z分数：$z = \frac{x-\mu}{\sigma}$
  - 平均绝对偏离度：$s = \frac{1}{n}\sum_{i=1}^{n}|x_i-m|$

- **闵可夫斯基距离**：
  - 曼哈顿距离(L1)：$d(i,j) = \sum_{k=1}^{p}|x_{ik}-x_{jk}|$
  - 欧几里得距离(L2)：$d(i,j) = \sqrt{\sum_{k=1}^{p}(x_{ik}-x_{jk})^2}$
  - 上确界距离(L∞)：$d(i,j) = \max_k|x_{ik}-x_{jk}|$

#### 4.2.4 序数属性距离
- 把数值属性的值域映射到具有$M_f$个状态的序数属性
- 序数标准化：$z_{if} = \frac{r_{if}-1}{M_f-1}$

#### 4.2.5 混合类型属性距离
$d(i,j) = \frac{\sum_{f=1}^{p}\delta_{ij}^{(f)}d_{ij}^{(f)}}{\sum_{f=1}^{p}\delta_{ij}^{(f)}}$

### 4.3 特殊类型数据的相似性度量

#### 4.3.1 余弦相似性距离
$\cos(d_1, d_2) = \frac{(d_1 \cdot d_2)}{||d_1|| \cdot ||d_2||}$

#### 4.3.2 类别型数据
- **逆文档频率(IDF)或Goodall度量**：
  $S(x_i,y_i) = \begin{cases} 
  \frac{1}{p_k(x_i)^2} & \text{如果} x_i = y_i \\
  1 - \frac{1}{p_k(x_i)^2} & \text{否则}
  \end{cases}$

#### 4.3.3 文本相似性度量
- **余弦度量**：
  $\cos(\vec{X}, \vec{Y}) = \frac{\sum_{i=1}^{d}x_i \cdot y_i}{\sqrt{\sum_{i=1}^{d}x_i^2} \cdot \sqrt{\sum_{i=1}^{d}y_i^2}}$
- **TF-IDF**：
  - 逆文档频率：$idf_i = \log(\frac{n}{n_i})$
  - 阻尼函数：$f_{x_i} = \log(x_i)$
  - 权重：$h_{x_i} = idf_i \cdot f_{x_i}$

#### 4.3.4 时态相似性度量
- **动态时间规整距离(DTW)**：
  - 按照距离最近的原则，构建两个序列元素之间的对应关系
  - 要求：单向对应，不能回头，不能有空，对应之后距离最近

#### 4.3.5 图的相似性度量
- **度量两个节点之间的相似性**
  - 基于结构距离的度量(连接路径的长短)
  - 基于随机游走的度量(连接路径的多少)
  
- **度量两个图之间的相似性**
  - 最大公共子图距离
  - 基于子结构的相似性
  - 基于图编辑的距离

### 4.4 距离函数设计考虑因素
- **维度**：$l_p$范数，p∈(0,1)的分数度量在高维情况下更有效
- **全局分布**：马哈拉诺比斯距离、ISOMAP
- **局部分布**：共享最近邻相似度
- **数据类型**：类别、数值、文本、时序、图

---

## 第五章 频繁模式、关联和相关性

### 5.1 基本概念

#### 5.1.1 基本定义
- **事务T**：项i的集合，$T=\{i_a,i_b,\dots,i_t\}$
- **数据集D**：T的集合
- **项集**：项的集合
- **k项集**：k个项的集合
- **关联规则**：$P \Rightarrow Q$，其中$P \cap Q = \emptyset$

#### 5.1.2 度量指标
- **项(项集)支持度**：$Support(X) = \frac{\#X}{n}$
- **关联规则支持度**：$Support(X \Rightarrow Y) = \frac{\#(X \cup Y)}{n}$
- **关联规则置信度**：$Confidence(X \Rightarrow Y) = \frac{\#(X \cup Y)}{\#(X)} = \frac{Support(X \cup Y)}{Support(X)} = P(Y|X)$

#### 5.1.3 重要概念
- **最小支持度σ**
- **最小置信度Φ**
- **频繁项集**：支持度大于σ的项集
- **强规则**：频繁且置信度大于Φ的规则

### 5.2 频繁项集挖掘方法

#### 5.2.1 Apriori算法
- **基本思想**：
  - 任何非频繁项集的超集都是不频繁的
  - 频繁项集的所有非空子集也一定是频繁的

- **算法过程**：
  1. 扫描数据库寻找k频繁项集
  2. 频繁k项集组合成频繁候选k+1项集
  3. 从1到k迭代，避免产生重复的不频繁项集
  4. 多次扫描数据库

- **连接步骤**：$l_{k-1} \times l_{k-1} \rightarrow c_k$
- **剪枝步骤**：删除$(k-1)$-子集不在$L_{k-1}$中的候选k-项集

#### 5.2.2 提高Apriori算法效率的方法
- **基于散列的技术**：减少候选集的数量
- **事务压缩**：不包含任何k项集的事务，不可能包含任何(k+1)项集
- **划分**：把数据库分成不重叠的区域
- **抽样**：在给定数据的一个子集挖掘
- **动态项集计数**

#### 5.2.3 FP-Growth算法
- **基本思想**：
  - 采用分治策略，挖掘频繁项集
  - 无需候选产生过程
  - 将数据库压缩到频繁模式树
  - 将压缩后的数据库分成条件数据库

- **算法流程**：
  1. 扫描数据，得到所有频繁一项集的计数
  2. 将读到的原始数据剔除非频繁1项集，按支持度降序排列
  3. 读入排序后的数据集，按顺序插入FP树中
  4. 从项头表的底部项依次向上找到项头表项对应的条件模式基
  5. 从条件模式基递归挖掘得到项头表项满足项数要求的频繁项集

#### 5.2.4 Eclat算法
- **要点**：使用垂直数据格式挖掘频繁项集
- **算法过程**：
  1. 通过扫描一次数据集，把水平格式的数据转换成垂直格式
  2. 项集的支持度计数等于项集的TID集的长度
  3. 从k=1开始，使用频繁k项集构造候选(k+1)项集
  4. 通过取频繁k项集的TID集的交，计算对应的(k+1)项集的TID集

### 5.3 频繁模式压缩

#### 5.3.1 闭频繁项集
- **定义**：如果不存在X的真超项集Y使得Y与X在D中具有相同的支持度计数，则称X在D中是closed的
- **特点**：挖掘闭频繁项集可显著减少频繁模式挖掘产生的模式数量，保持完整信息

#### 5.3.2 极大频繁项集
- **定义**：如果不存在频繁项集X的超项集Y，使得Y在D中是频繁的，则称X是D中的极大频繁项集

### 5.4 关联规则评价

#### 5.4.1 提升度和兴趣度
- **提升度**：$Lift(A \Rightarrow B) = \frac{confidence(A \Rightarrow B)}{p(B)} = \frac{P(A \cap B)}{P(A) \times P(B)}$
- **兴趣度**：$Interest(A \Rightarrow B) = \frac{P(A \cap B)}{P(A) \times P(B)} - 1$

#### 5.4.2 KULC度量 + 不平衡比(IR)
- 用于处理不平衡数据中的关联规则评价

### 5.5 多维与多层关联规则

#### 5.5.1 多维关联规则挖掘
- **基于聚类挖掘量化关联规则**：有趣的模式通常在量化属性稠密的簇中发现

#### 5.5.2 多层关联规则挖掘
- **一致支持度**：对所有层都使用一致的最小支持度
- **递减支持度**：在较低层使用递减的最小支持度

### 5.6 近似频繁模式

#### 5.6.1 通过模式聚类挖掘模式
- 距离测量
- δ-聚类：找到距离P的距离小于δ的类$p_i$

#### 5.6.2 提取感知冗余的top-k模式
- **显著性**：$s_{p,q} = s_{p,q} - s_q$
- **冗余性**：$R_{p,q} = S_p + S_q - S(p,q)$

### 5.7 序列模式

#### 5.7.1 基本概念
- **定义**：是一个有序的元素列表，其中每个元素是一个或多个项目的集合
- **子序列**：如果存在整数$1 \leq j_1 < j_2 < \dots < j_m \leq n$使得$t_1 \subseteq s_{j1}, t_2 \subseteq s_{j2}, \dots, t_m \subseteq s_{jm}$，则$t=<t_1 t_2 \dots t_m>$是$s=<s_1 s_2 \dots s_n>$的子序列

#### 5.7.2 序列模式挖掘
- 候选集生成效率问题
- 类Apriori的序列模式挖掘算法

---

## 第六章 聚类

### 6.1 聚类基本概念

#### 6.1.1 什么是聚类
- **定义**：将数据集划分为若干个组(簇)，使得同一组内的数据对象相似度高，不同组间的数据对象相似度低
- **好的聚类方法**：产生高类内相似性、低类间相似性的聚类结果

#### 6.1.2 聚类应用
- 生物学：生物分类
- 信息检索：文档聚类(web搜索)
- 土地利用：识别相似的土地利用区域
- 营销：为对应的客户制定相应的营销计划
- 城市规划：城市功能的科学分类和定量评估
- 地震研究：发掘相似地震
- 气候：理解气候，发现大气和海洋模式
- 图像识别
- 预处理：作为其他算法的预处理步骤
- 离群点检测

#### 6.1.3 聚类分析要素
- **划分准则**：单层vs层次划分
- **簇的分离性**：互斥vs一对多
- **相似性度量**：距离度量vs连接性度量
- **聚类空间**：全空间vs子空间

#### 6.1.4 聚类方法的基本要求
- 可伸缩性
- 对领域知识要求最小化
- 最小化基于知识领域确定的输入参数敏感性
- 可解释性和可用性
- 可发掘任意形状的类别
- 抗噪声
- 增量聚类和对输入次序不敏感
- 聚类高维数据的能力

### 6.2 主要聚类方法

#### 6.2.1 基于代表点的算法
- **K-means算法**：
  - 目标函数：$E = \sum_{i=1}^{k} \sum_{p \in C_i} ||p - c_i||^2$
  - 优点：简单，收敛快
  - 缺点：需要制定类别个数，可能收敛到局部最优，适用于球形簇，对噪声和离群点敏感
  
- **K-medians算法**：
  - 目标函数：$E_1 = \sum_{i=1}^{k} \sum_{p \in C_i} |p - m_i|$
  - 最佳代表点是簇每一维度代表点的中值
  - 相比K-means，对异常点不那么敏感

- **K-medoids算法**：
  - 从数据集D中选取代表点
  - 步骤：
    1. 从D中选择代表点形成代表点集合S的初始值
    2. 用距离函数为每个数据点找到集合S中最近的代表点
    3. 找到D中的数据点以及S中的代表点，使得用代替S中的后目标函数的改进最大
    4. 只有上述改进为正时，才将S中的替换成
    5. 循环直到当前迭代无任何改进

#### 6.2.2 层次聚类算法
- **自底向上(凝聚)方法**：
  - 独立的数据点逐步凝聚成高层次的簇
  - 距离计算方法：
    - 最优链接(单链接)：取所有成员对间距离的最小值
    - 最差链接(全链接)：取所有成员对间距离的最大值
    - 组平均链接：取所有成员对间距离的平均值
    - 基于方差的方法：使得合并后目标函数的变化最小化

- **自顶向下(分裂)方法**：
  - 将数据点连续划分成树状结构
  - 可作为通用元方法，几乎可以使用所有的聚类算法作为子程序

- **BIRCH算法**：
  - 利用层次结构的平衡迭代规约和聚类
  - 阶段1：引入CF树的层次数据结构
  - 阶段2：基于叶子结点进行聚类
  - 重要参数：枝平衡因子β、叶平衡因子λ、空间阈值τ
  - CF(聚类特征)：用三元组(N, LS, SS)概括描述各簇的信息

- **Chameleon算法**：
  - 本质上是一个从下而上的层次聚类算法
  - 只考虑每个节点邻近的K个节点
  - 使用动态建模确定一对簇之间的相似度
  - 步骤：
    1. 使用"图划分算法"将"k临近图"划分为大量相对较小的子簇
    2. 使用"凝聚层次聚类算法"，基于子簇的相似度反复合并子簇

#### 6.2.3 基于密度的算法
- **DBSCAN算法**：
  - 发现任意形状簇，抗噪声，无需事先指定K值
  - 基本概念：
    - 密度：邻域内的对象数
    - 核心点：高密度的点(邻域内至少包含k个样本点)
    - 边界点：低密度点但是在核心点的邻域内
    - 噪声点：既不是核心点也不是边界点

- **OPTICS算法**：
  - Ordering Points To Identify the Clustering Structure
  - 将eps指定为一个范围，而非一个固定值
  - 计算一个簇次序，代表数据的基于密度的聚类结构
  - 基本概念：
    - 核心距离：使得x成为核心点的最小邻域半径
    - 可达距离：假设p是核心点，点p和q的可达距离定义为$max(core\_distance(p), distance(p,q))$

- **DENCLUE算法**：
  - 引入影响函数和密度函数的概念
  - 空间中任一点的密度是所有数据点在此点产生影响的叠加
  - 使用密度吸引子将数据划分成簇

#### 6.2.4 基于网格的算法
- **STING**：
  - 统计信息网格方法
  - 用多分辨率的网格结构表达原始数据
  - 将空间区域划分成网格单元
  - 不同层次的网格表示不同分辨率下的数据特征

- **CLIQUE**：
  - 基于密度和网格的算法
  - 把每一维都划分成等长的区间
  - 将m维的数据划分成互不重叠的长方形子区间
  - 如果一个区间中的数据点个数超过阈值，则认为该区间是稠密单元

#### 6.2.5 基于概率模型的算法
- **EM算法**：
  - 软聚类算法
  - 假设数据由数据分布为$\theta_k$的k个分布混合生成
  - 每个分布代表一个簇，也称为混合分量
  - 步骤：
    1. E步骤：将之前未见到的点分配给某一个类别
    2. M步骤：将概率分布的参数进行优化

#### 6.2.6 基于图的方法
- **谱聚类**：
  - 将数据转化成图进行分析
  - 将图映射到多维空间中进行切分
  - 聚类问题转化为切图问题，使得切图后的总代价最小
  - 标准化的谱聚类算法流程：
    1. 计算邻接矩阵w和度矩阵
    2. 计算标准化的拉普拉斯矩阵
    3. 计算矩阵的k个最小特征值对应的n维单位向量
    4. k个n维特征向量组成n×k维的矩阵M
    5. 对该n个样本进行k均值聚类算法，得到聚类结果

### 6.3 聚类评估

#### 6.3.1 数据的聚集程度评估
- **熵**：可以反馈特征子集的聚类质量
- **霍普金斯统计**：
  - $H = \frac{\sum_{i=1}^{m}u_i}{\sum_{i=1}^{m}u_i + \sum_{i=1}^{m}w_i}$
  - 均匀分布时，H值为0.5；聚集时，H值大于0.5

#### 6.3.2 K值确定
- **经验法**：
- **"肘"方法**：对n个点的数据集，迭代计算k from 1 to n，计算每个点到其所属簇中心的距离的平方和
- **交叉验证**

#### 6.3.3 聚类质量
- **内部验证度量**：
  - **Silhouette系数**：
    - $a(i)$：对象i与同一簇中其他对象的平均距离
    - $b(i)$：对象i与不同簇中对象的平均距离的最小值
    - $s(i) = \frac{b(i)-a(i)}{max\{a(i),b(i)\}}$
    - Silhouette系数接近1表示聚类效果好

- **外部验证度量**：
  - **纯度**：$\frac{1}{N}\sum_k max_j |cluster_k \cap class_j|$
  - **基尼系数**：$1 - \sum_j (\frac{|cluster_k \cap class_j|}{|cluster_k|})^2$

---

## 第七章 离群点检测

### 7.1 基本概念

#### 7.1.1 离群点定义
- **Hawkins定义**：离群点是在数据集中偏离大部分数据的数据，使人怀疑这些数据的偏离并非由随机因素产生，而是完全产生于不同的机制
- **Weisberg定义**：离群点是与数据集中其余部分不服从相同统计模型的数据
- **Samuels定义**：离群点是数据集中足够地不同于数据集中其余部分的数据
- **Porkess定义**：离群点是远离数据集中其余部分的数据

#### 7.1.2 噪声vs离群点
- **噪声**：是一个测量变量中的随机错误或偏差，包括错误的值，偏离期望的孤立点

#### 7.1.3 离群点类型
- **全局离群点**
- **局部离群点**：情境(条件)离群点
- **集体离群点**：数据对象的子集形成集体离群点

#### 7.1.4 应用
- 欺诈检测
- 灾害预报
- 嫌疑人判定
- 群体检测(疫情)
- 罕见病检测

### 7.2 离群点检测方法

#### 7.2.1 基于密度的方法
- **直方图**：假设每个维度间都是独立的，分别计算一个样本在不同维度上所处的密度空间，并叠加结果
- **核密度估计**：
  - $f(\vec{x}) = \frac{1}{n}\sum_{i=1}^{n}K(\vec{x} - \vec{x}_i)$
  - $K(\vec{x} - \vec{x}_i) = \frac{1}{\sigma\sqrt{2\pi}^d}e^{-\frac{||\vec{x} - \vec{x}_i||^2}{2\sigma^2}}$

#### 7.2.2 基于概率的方法
- **基本思想**：识别模型低概率区域中的对象
- **一元离群点检测**：
  - 密度分布函数$f(x;x;\theta)$
  - $f(x;x;\theta) \leq \theta$
  
- **多元离群点检测**：
  - $f(\vec{x}) = \frac{1}{|\Sigma| \cdot (2 \cdot \pi)^{d/2}} \cdot e^{-\frac{1}{2}(\vec{x} - \vec{\mu})\Sigma^{-1}(\vec{x} - \vec{\mu})^T}$
  - $Mahalanobis(\vec{x}, \vec{\mu}, \Sigma)^2$

- **使用混合参数分布**：
  - 构建似然函数
  - 通过迭代的EM算法求解模型参数和概率分布

#### 7.2.3 基于距离的方法
- **基本思路**：数据集中显著偏离其它对象的点是离群点
- **判定依据**：
  - A. 给定邻域半径，o的r邻域所包含的对象个数占比
  - B. 依据o与其第k个邻近之间的距离(或平均距离)判定o是否离群

- **基于距离的离群点检测算法**：
  - 输入：对象集D，阈值r和π
  - 输出：D中的DB(r,π)离群点

- **局部距离的修正**：
  - 从o'到o的可达距离：$reach\_dist_k(o,o') = max\{k\_distance(o), distance(o,o')\}$
  - o的局部可达密度：$lrd_k(o) = \frac{1}{\sum_{o' \in N_k(o)} \frac{reach\_dist_k(o,o')}{|N_k(o)|}}$
  - 对象o的局部离群点因子LOF：$LOF_k(o) = \frac{\sum_{o' \in N_k(o)} \frac{lrd_k(o')}{lrd_k(o)}}{|N_k(o)|}$

#### 7.2.4 基于聚类的方法
- **基本思想**：
  - 建立正常模型
  - 离群点为不能正常地符合这个模型的数据点
  - 将异常程度量化为数值(异常分)
  
- **形式**：
  - 基于聚类产生簇，寻找远离簇的数据点
  - 考虑对象和它最近簇之间的距离
  - 不属于任何Cluster或远离任何Cluster

- **FindCBLOF**：检测小簇中的离群点
  - 聚类，将类别降序排序
  - 赋予每个点一个基于簇的离群点检测因子(CBLOF)

#### 7.2.5 基于分类的方法
- **基本思想**：训练一个可以区分正常数据离群点的分类模型
- **One-Class模型**：
  - 与二分类和多分类的区别
  - 将不属于该类的所有其他样本判别为"不是"
  
- **One-Class SVM——SVDD**：
  - 采用一个超球体而不是一个超平面来做划分
  - 在特征空间中获得数据周围的球形边界
  - 期望最小化这个超球体的体积

- **半监督学习**：结合聚类和分类检测离群点

#### 7.2.6 其它方法
- **Isolation Forest**：
  - 思路：异常样本相较普通样本可以通过较少次数的随机特征分割被孤立出来
  - 利用决策树的特点，多次对特征空间进行划分，观察"孤立"一个点难易程度
  - 异常点往往在决策树的深度比较低的叶子结点

- **自编码器(Autoencoder)**：
  - 思路：通过自编码器的先压缩(encoder)再还原(decoder)，异常点会有更大的重建误差

### 7.3 模型集成与评价

#### 7.3.1 多模型合并
- **直接合并**：缺点是结果平庸
- **动态模型选择**：
  - 对于每个测试点生成局部空间，也即近邻
  - 模型选择与合并

#### 7.3.2 模型评价
- **外部度量**：将从人工合成数据集或真实数据集的罕见类别中获得的已知异常点标签作为真值
- **ROC曲线**：处理假正例和假负例平衡

---

## 挖掘Web数据：PageRank算法

### 10.1 Web作为有向图
- Web页面可以看作有向图中的节点
- 超链接可以看作有向边

### 10.2 PageRank基本思想
- **链接作为投票**：
  - 页面更重要如果它有更多链接
  - 入链作为投票
  - 来自重要页面的链接计票更多
  - 递归问题

### 10.3 PageRank数学模型

#### 10.3.1 流动模型
- 每个链接的投票与源页面重要性成正比
- 如果页面j有重要性$r_j$和$n$个出链，每个链接得到$r_j/n$票
- 页面j的重要性是入链投票之和：$r_j = \sum_{i \to j} \frac{r_i}{d_i}$

#### 10.3.2 矩阵形式
- **随机邻接矩阵M**：
  - 如果页面i有$d_i$个出链
  - 如果$i \to j$，则$M_{ji} = \frac{1}{d_i}$，否则$M_{ji} = 0$
  - M是列随机矩阵，列和为1

- **秩向量r**：每个页面对应一个重要性得分
- 流动方程：$r = M \cdot r$

#### 10.3.3 特征向量形式
- 秩向量r是随机web矩阵M的特征向量
- 对应特征值为1的主特征向量

### 10.4 PageRank计算方法

#### 10.4.1 幂迭代方法
- 初始化：$r^{(0)} = [1/N,....,1/N]^T$
- 迭代：$r^{(t+1)} = M \cdot r^{(t)}$
- 停止条件：$|r^{(t+1)} - r^{(t)}|_1 < \epsilon$

#### 10.4.2 问题与解决方案
- **问题1：死端(Dead ends)**：没有出链的页面导致重要性"泄漏"
- **问题2：蜘蛛陷阱(Spider traps)**：所有出链在组内，随机游走被"困住"

- **解决方案：随机传送**：
  - 在每一步，随机冲浪者有两个选择：
    - 以概率β，随机跟随一个链接
    - 以概率1-β，跳转到某个随机页面
  - 常见β值范围：0.8到0.9

#### 10.4.3 Google矩阵
- **PageRank方程**：$r_j = \sum_{i \to j} \frac{\beta r_i}{d_i} + \frac{1-\beta}{N}$
- **Google矩阵A**：$A = \beta M + \frac{1-\beta}{N}N \times N$
- 递归问题：$r = A \cdot r$

### 10.5 主题特定PageRank
- **目标**：评估Web页面不仅根据通用流行度，还根据与特定主题的接近程度
- **方法**：
  - 随机游走者有很小概率在任何步骤传送
  - 传送到主题相关的"相关"页面集合(传送集)
  - 偏向随机游走
  - 当游走者传送时，从集合S中选择一个页面
  - 对于每个传送集S，得到不同的向量$r_S$

- **矩阵形式**：
  - 更新传送部分的PageRank公式：
    - $A_{ij} = \beta M_{ij} + \frac{1-\beta e_s}{|S|}$ 如果$i \in S$
    - $A_{ij} = \beta M_{ij} + 0$ 否则

---

## 考试复习要点

### 1. 计算题重点
- 数据预处理中的标准化计算
- 距离和相似性度量计算
- 关联规则支持度和置信度计算
- Apriori算法候选集生成和剪枝
- K-means聚类过程
- PageRank计算

### 2. 简答题重点
- 各章节基本概念和定义
- 各种算法的基本思想和流程
- 算法间的比较和优缺点
- 方法适用场景

### 3. 论述分析题重点
- 针对具体案例应用大数据挖掘思路进行完整阐述
- 从方法到评价的全面分析
- 经典方法的原理、运行流程、方法间根本不同和关键改进

### 4. 特别注意事项
- 闭频繁和极大频繁需要掌握
- 近似频繁模式了解即可
- PCA需要理解原理和用途
- 推荐算法章节不在考试范围
- PPT上的例题要能换数据计算

---

*本文档根据课程PPT内容和老师提示消息整理，涵盖了"大数据挖掘与分析"课程的主要知识点，重点突出了考试相关内容。*