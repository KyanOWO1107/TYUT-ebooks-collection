# 大数据挖掘与分析知识点汇总

## 考试重点提示
根据老师消息，考试有以下特点：
- 今年题偏难，包括选择、简答、计算和论述分析题
- 推荐算法章节不在考试范围，其他章节都有涉及
- PPT上有例子的都要弄明白，保证换个数据能做出来
- 从方法到评价要复习全面
- 课上强调的经典方法需要准确掌握运行流程，能写出流程，能算例题
- 需要掌握经典方法间的优势、不同和关键改进
- 不常用方法需掌握基本思想
- 针对具体案例能完整应用大数据挖掘思路进行阐述
- 闭频繁和极大频繁需掌握，具体到近似频繁模式了解即可
- PCA需要理解，知道干什么用的，做什么，原理是什么

---

## 第一章 绪论

### 1.1 数据挖掘定义与基本概念
- **数据挖掘定义**：从大量的数据中挖掘那些令人感兴趣的、有用的、隐含的、先前未知的和可能有用的模式或知识
- **数据分析定义**：指用适当的统计分析方法对收集来的大量数据进行分析，将它们加以汇总和理解并消化，以求最大化地开发数据的功能，发挥数据的作用
- **数据挖掘与分析目的**：揭示事实，发现趋势，预测未来

### 1.2 数据挖掘应用领域
- 公共安全：犯罪分析
- 医疗保健领域
- 城市规划
- 位置信息分析
- 情感分析
- 社交网络分析
- 精准销售
- 海平面温度预测

### 1.3 数据挖掘过程

#### 1.3.1 CRISP-DM过程模型
**性质**：跨行业数据挖掘过程标准，业界广泛采用的业务导向数据挖掘方法论

**详细流程**：
1. **业务理解**：
   - 理解业务目标和需求
   - 评估现状和资源
   - 确定数据挖掘目标
   - 制定项目计划

2. **数据理解**：
   - 收集原始数据
   - 描述数据
   - 探索数据
   - 验证数据质量

3. **数据准备**：
   - 选择数据
   - 清洗数据
   - 构建数据
   - 整合数据

4. **建模**：
   - 选择建模技术
   - 生成测试设计
   - 构建模型
   - 评估模型

5. **评估**：
   - 评估结果
   - 审查过程
   - 确定下一步

6. **部署**：
   - 计划部署
   - 计划监控和维护
   - 产生最终报告
   - 项目回顾

#### 1.3.2 本书介绍的数据挖掘过程
1. **挖掘目标的定义**
   - 需要解决问题的明确定义
   - 对有关数据的了解
   - 数据挖掘结果对业务作用效力的预测
   - 从问题定义和效果评估两个角度定义目标

2. **数据的准备**
   - 数据的选择：从数据源中搜索所有与业务对象有关的数据
   - 数据的质量分析：评估数据质量
   - 数据的预处理：数据清洗、数据集成、数据归约和数据变换

3. **数据的探索**
   - 描述统计：均值、频率、众数、百分位数、中位数、极差、方差等
   - 数据的可视化：频次图、散点图、箱体图等
   - 数据探索的建模活动

4. **模型的建立**
   - 模型结构：分类、聚类、回归拟合、关联规则、时序模型、异常点检测等
   - 模型操作流程

5. **模型的评估**
   - 模式兴趣度度量：客观度量(支持度、置信度等)和主观度量
   - 评估指标：精确度、LIFT、ROC、Gain图等

6. **模型的部署**
   - 提供给分析人员做参考
   - 开发并部署到实际的业务系统中

#### 1.3.3 餐饮行业案例分析详解
**性质**：实际应用案例，展示完整数据挖掘流程

**详细步骤**：
1. **定义目标**：
   - 增加盈利能力→凝聚优质老客户，吸引更多新客户
   - 降低运营成本→减少食材浪费，菜品受欢迎程度预测
   - 扩大经营→选新址建新店
   - 具体目标：
     * 细分餐饮客户，用户画像
     * 面向用户的菜品智能推荐
     * 基于菜品历史销售情况，原材料需求预测
     * 优化新店选址

2. **数据准备**：
   - 采集标准：相关性、可靠性、有效性
   - 采集方式：随机抽样、等距抽样、分层抽样、分类抽样、全收集
   - 内部数据：菜品采购情况，餐饮销售情况，成本单价，会员消费，促销活动
   - 外部数据：天气，节假日，竞争对手，周边环境

3. **数据预处理**：
   - 数据缺点：噪音多，不完整，不一致
   - 处理目的：数据一致准确，完整无缺失，维度低方便处理
   - 处理方法：
     * 数据清洗：数据筛选，缺失值处理，数据对齐，属性选择
     * 数据变换：数据标准化，数据转换，数据规约，主成分分析

4. **数据探索**：
   - 考虑因素：
     * 是否达到原来设想的要求
     * 样本中有没有明显的规律和趋势
     * 有没有出现从未设想过的数据状态
     * 属性之间的相关性
   - 分析处理方法：周期性分析，异常值分析，相关分析

5. **问题建模**：
   - 模型类型：分类，聚类，回归拟合，关联规则，时序模型，异常点检测
   - 餐饮行业问题模型：
     * 基于聚类算法的餐饮客户价值分析
     * 基于关联规则算法的动态菜品智能推荐
     * 基于分类/回归预测算法的菜品销量预测
     * 基于整体优化的新店选址

---

## 第二章 数据准备

### 2.1 认识数据

#### 2.1.1 数据定义与特征
- **数据定义**："Data are pieces of information that represent the qualitative or quantitative attributes of a variable or set of variables"
- **数据矩阵**：通常数据可以抽象成一个n×d的数据矩阵
  - 行：实体、实例、样本、记录、事务、对象、数据点、特征向量和元组等
  - 列：属性、性质、特征、维度、域等

#### 2.1.2 属性类型
- **数值型(Numeric)**
  - 区间标度：只有差值有意义(如温度)
  - 比例标度：差值和比例都有意义(如年龄)
- **类别型(Categorical)**
  - 名义类(Nominal)：只有相同有意义，包括二元和多元
  - 序数类(Ordinal)：有顺序关系(如教育水平)

#### 2.1.3 数据类型
- **记录数据**：关系记录、文本数据、事务数据
- **图形和网络数据**：万维网、社交网络
- **有序数据**：时间数据、序列数据
- **空间、图像和多媒体数据**：空间数据、图像数据、视频数据

#### 2.1.4 数据的几何和代数描述
- 数据集中每一行可以看作d维空间的点或d维空间列向量
- 数据矩阵平均：$\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$
- 中心数据矩阵：$X_c = [\bar{x}, x_2-\bar{x}, ..., x_n-\bar{x}]$
- 点乘：$x \cdot y = \sum_{i=1}^{d}x_i y_i$
- 欧几里得范数：$||x|| = \sqrt{x \cdot x}$
- 两向量间的距离：$d(x,y) = ||x-y||$
- 两向量间的夹角：$\cos\theta = \frac{x \cdot y}{||x|| \cdot ||y||}$

### 2.2 数据质量分析

#### 2.2.1 数据质量问题
- **不完整**：缺失值(如职业="")
- **噪声**：数据错误(如工资="-100")
- **不一致**：编码不一致(如年龄="42" VS 生日="08/08/1982")

#### 2.2.2 数据质量分析指标
- 总记录数：表征数据规模
- 唯一值数：表征数据多样性
- 空值占比：表征无效数据的影响程度
- 非零占比：表征非零值的影响程度
- 正数占比：表征正值的影响程度
- 负数占比：表征负值的影响程度

### 2.3 数据预处理

#### 2.3.1 数据清洗

##### 缺失值处理算法
**性质**：数据清洗核心技术，处理不完整数据

**算法分类**：
1. **删除法**：
   - 删除含有缺失值的记录
   - 删除含有缺失值的属性
   - 适用：缺失比例小(<5%)

2. **填充法**：
   - **固定值填充**：用特定值(如0、-1)填充
   - **均值填充**：用属性均值填充
     - 数值型：算术平均值
     - 分类型：众数
   - **中位数填充**：用中位数填充，适用于偏态分布
   - **回归填充**：基于其他属性预测缺失值
   - **K近邻填充**：用相似样本的值填充

3. **插补法**：
   - **均值插补**：属性存在值的平均值或众数
   - **回归插补**：建立回归模型预测缺失值
   - **极大似然估计**：假设模型正确，用边际分布估计参数

##### 噪声数据处理算法
**性质**：数据清洗核心技术，处理错误或偏差数据

**算法分类**：
1. **回归法**：
   - 用函数拟合数据来光滑数据
   - 线性回归、多项式回归等
   - 适用：数据有明显趋势

2. **均值平滑法**：
   - 对于具有序列特征的变量用临近若干数据的均值替换
   - 移动平均、指数平滑等
   - 适用：时间序列数据

3. **离群点分析**：
   - **局部离群因子(LOF)**：
     - 公式：$LOF_k(p) = \frac{\sum_{o \in N_k(p)} \frac{lrd_k(o)}{lrd_k(p)}}{|N_k(p)|}$
     - $lrd_k(p) = \frac{1}{\sum_{o \in N_k(p)} \frac{reach\_dist_k(p,o)}{|N_k(p)|}}$
     - $reach\_dist_k(p,o) = max\{k\_distance(p), distance(p,o)\}$
   - 适用：高维数据、局部离群点检测

#### 2.3.2 数据集成
- **实体识别**：匹配来自不同数据源的现实世界的实体
- **属性集成**：对同一实体的不同属性值进行整合
- **冗余性识别**：检测和处理冗余数据

#### 2.3.3 数据规约

##### 特征选择算法
**性质**：数据规约技术，减少数据维度

**算法分类**：
1. **过滤法(Filter)**：
   - 基于数据统计特性选择特征
   - 方法：相关系数、卡方检验、信息增益、方差选择
   - 特点：独立于学习算法，计算效率高

2. **包装法(Wrapper)**：
   - 基于学习算法性能选择特征
   - 方法：递归特征消除、前向选择、后向消除
   - 特点：考虑特征间相互作用，计算成本高

3. **嵌入法(Embedded)**：
   - 特征选择嵌入学习算法过程
   - 方法：LASSO、树模型特征重要性
   - 特点：平衡效率和效果

##### 特征提取算法
**性质**：数据规约技术，创建新特征表示

**算法分类**：
1. **主成分分析(PCA)**：
   - 目标：最大化投影方差
   - 步骤：
     * 数据标准化
     * 计算协方差矩阵
     * 计算特征值和特征向量
     * 选择主成分
   - 适用：线性相关特征提取

2. **线性判别分析(LDA)**：
   - 目标：最大化类间散度，最小化类内散度
   - 公式：$J_w = \frac{w^T S_B w}{w^T S_W w}$
   - 适用：有监督降维，分类任务

3. **独立成分分析(ICA)**：
   - 目标：寻找统计独立的成分
   - 适用：盲源分离、信号处理

#### 2.3.4 数据变换

##### 数据标准化算法
**性质**：数据变换技术，消除量纲影响

**算法分类**：
1. **最小-最大标准化**：
   - 公式：$v' = \frac{v-min}{max-min} \times (new\_max-new\_min) + new\_min$
   - 特点：保持原始数据分布形状
   - 适用：已知数据边界

2. **Z-score标准化**：
   - 公式：$v' = \frac{v-\mu}{\sigma}$
   - 特点：标准化后均值为0，标准差为1
   - 适用：数据近似正态分布

3. **小数定标标准化**：
   - 公式：$v' = \frac{v}{10^j}$，其中j是使max(|v'|)<1的最小整数
   - 特点：保持数据的小数位数
   - 适用：数据值范围大但分布相对均匀

- **离散化**
  - 等宽离散化
  - 等频离散化
  - 基于聚类的离散化

- **语义转换**
  - 将字符型属性值转换为整型数据

---

## 第三章 数据探索

### 3.1 衍生变量
- **定义**：由其它既有变量通过不同形式的组合而衍生出的变量
- **原则**：
  - 衍生变量能够客观反映事物的特征
  - 衍生变量与数据挖掘的业务目标有一定的联系
- **方法**：
  - 对多个列变量进行组合
  - 按照维度分类汇总
  - 对某个变量进一步分解
  - 对具有时间序列特征的变量提取时序特征

#### 3.1.1 衍生变量构造方法
**性质**：特征工程技术，创建有意义的组合特征

**构造方法**：
1. **算术组合**：
   - 加减乘除：身高/体重(BMI)、负债/收益
   - 比例：总通话时间/总呼叫次数
   - 差值：终值-初值

2. **维度分类汇总**：
   - 按类别分组统计：手机型号分组的流失率
   - 时间窗口统计：最近7天活跃度
   - 地理区域统计：区域销售额占比

3. **变量分解**：
   - 时间分解：年、季度、月、日、星期、是否节假日、工作日、周末
   - 文本分解：词频、TF-IDF、n-gram
   - 地址分解：省、市、区、街道

4. **时序特征提取**：
   - 滑动窗口：移动平均、移动标准差
   - 趋势特征：增长率、加速度
   - 周期特征：季节性指数、傅里叶变换

### 3.2 数据的统计

#### 3.2.1 基本描述统计
- **表示位置的统计量**
  - 算术平均值：$\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$
  - 中位数：将数据由小到大排序后位于中间位置的数值
  - 众数：出现频率最高的值

- **表示数据散度的统计量**
  - 标准差：$s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2}$
  - 方差：$s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2$
  - 极差：$range = max(x) - min(x)$

- **表示分布形状的统计量**
  - 偏度：反映分布的对称性
  - 峰度：用作衡量偏离正态分布的尺度

#### 3.2.2 多元汇总统计
- **多元数据的位置度量**：$\bar{X} = (\bar{x}_1, \cdots, \bar{x}_d)$
- **多元数据的散布**
  - 协方差矩阵：$s_{ij} = cov(x_i, x_j)$
  - 相关矩阵：$r_{ij} = \frac{cov(x_i \cdot x_j)}{s_i s_j}$

### 3.3 数据可视化

#### 3.3.1 基本统计描述的图表示
- **正态分布曲线**：
  - 从μ-σ到μ+σ，包含68%的测量值
  - 从μ-2σ到μ+2σ，包含95%的测量值
  - 从μ-3σ到μ+3σ，包含99.7%的测量值

- **箱线图**：5数概括(最小值、Q1、中位数、Q3、最大值)
  - 四分位：Q1(25%)、Q3(75%)
  - 四分位极差：Q3-Q1
  - 离群点：大于/小于1.5×IQR的值

- **分位数图**：展示整体行为和异常行为
- **Q-Q图**：展示从一个分布到另一个分布的漂移
- **直方图**：比箱线图信息更丰富
- **散点图**：可以看成聚类或离群点的初探索，用于相关性分析和分类判断

#### 3.3.2 高级可视化技术
- **基于像素的可视化**：对m维数据创建m个窗口
- **几何投影的可视化**：
  - 直接可视化
  - 地形图
  - 投影追寻技术
  - 剖视图
  - 超切片
  - 平行坐标
- **等值线图**
- **平行坐标系**：k个平行的坐标表示属性
- **切尔诺夫脸**：用卡通脸表示数据特征
- **人物线条画**
- **树图**：把层次数据显示成嵌套矩形的集合

---

## 第四章 距离与相似性

### 4.1 基本概念
- **邻近性**：相似性和相异性统称为邻近性
- **属性类型**：标称属性、序数属性、数值属性
- **数据结构**：
  - 数据矩阵：存放数据对象
  - 相异性矩阵：存放数据对象的相异性值

### 4.2 不同类型数据的邻近性度量

#### 4.2.1 二元属性的临近性
- **对称的二元相异性**：所有状态同等重要
- **非对称的二元相异性**：非对称性，使用Jaccard系数

#### 4.2.2 标称属性的邻近性
- 直接度量
- 转化成二元属性：$d(i,j) = \frac{p-m}{p}$

#### 4.2.3 数值属性的相似性
- **数值数据的标准化**
  - Z分数：$z = \frac{x-\mu}{\sigma}$
  - 平均绝对偏离度：$s = \frac{1}{n}\sum_{i=1}^{n}|x_i-m|$

- **闵可夫斯基距离**：
  - 曼哈顿距离(L1)：$d(i,j) = \sum_{k=1}^{p}|x_{ik}-x_{jk}|$
  - 欧几里得距离(L2)：$d(i,j) = \sqrt{\sum_{k=1}^{p}(x_{ik}-x_{jk})^2}$
  - 上确界距离(L∞)：$d(i,j) = \max_k|x_{ik}-x_{jk}|$

#### 4.2.4 序数属性距离
- 把数值属性的值域映射到具有$M_f$个状态的序数属性
- 序数标准化：$z_{if} = \frac{r_{if}-1}{M_f-1}$

#### 4.2.5 混合类型属性距离
$d(i,j) = \frac{\sum_{f=1}^{p}\delta_{ij}^{(f)}d_{ij}^{(f)}}{\sum_{f=1}^{p}\delta_{ij}^{(f)}}$

### 4.3 特殊类型数据的相似性度量

#### 4.3.1 余弦相似性距离
$\cos(d_1, d_2) = \frac{(d_1 \cdot d_2)}{||d_1|| \cdot ||d_2||}$

#### 4.3.2 类别型数据
- **逆文档频率(IDF)或Goodall度量**：
  $S(x_i,y_i) = \begin{cases} 
  \frac{1}{p_k(x_i)^2} & \text{如果} x_i = y_i \\
  1 - \frac{1}{p_k(x_i)^2} & \text{否则}
  \end{cases}$

#### 4.3.3 文本相似性度量
- **余弦度量**：
  $\cos(\vec{X}, \vec{Y}) = \frac{\sum_{i=1}^{d}x_i \cdot y_i}{\sqrt{\sum_{i=1}^{d}x_i^2} \cdot \sqrt{\sum_{i=1}^{d}y_i^2}}$
- **TF-IDF**：
  - 逆文档频率：$idf_i = \log(\frac{n}{n_i})$
  - 阻尼函数：$f_{x_i} = \log(x_i)$
  - 权重：$h_{x_i} = idf_i \cdot f_{x_i}$

#### 4.3.4 时态相似性度量
- **动态时间规整距离(DTW)**：
  - 按照距离最近的原则，构建两个序列元素之间的对应关系
  - 要求：单向对应，不能回头，不能有空，对应之后距离最近

#### 4.3.5 图的相似性度量
- **度量两个节点之间的相似性**
  - 基于结构距离的度量(连接路径的长短)
  - 基于随机游走的度量(连接路径的多少)
  
- **度量两个图之间的相似性**
  - 最大公共子图距离
  - 基于子结构的相似性
  - 基于图编辑的距离

### 4.4 距离函数设计考虑因素
- **维度**：$l_p$范数，p∈(0,1)的分数度量在高维情况下更有效
- **全局分布**：马哈拉诺比斯距离、ISOMAP
- **局部分布**：共享最近邻相似度
- **数据类型**：类别、数值、文本、时序、图

---

## 第五章 频繁模式、关联和相关性

### 5.1 基本概念

#### 5.1.1 基本定义
- **事务T**：项i的集合，$T=\{i_a,i_b,\dots,i_t\}$
- **数据集D**：T的集合
- **项集**：项的集合
- **k项集**：k个项的集合
- **关联规则**：$P \Rightarrow Q$，其中$P \cap Q = \emptyset$

#### 5.1.2 度量指标
- **项(项集)支持度**：$Support(X) = \frac{\#X}{n}$
- **关联规则支持度**：$Support(X \Rightarrow Y) = \frac{\#(X \cup Y)}{n}$
- **关联规则置信度**：$Confidence(X \Rightarrow Y) = \frac{\#(X \cup Y)}{\#(X)} = \frac{Support(X \cup Y)}{Support(X)} = P(Y|X)$

#### 5.1.3 重要概念
- **最小支持度σ**
- **最小置信度Φ**
- **频繁项集**：支持度大于σ的项集
- **强规则**：频繁且置信度大于Φ的规则

### 5.2 频繁项集挖掘方法

#### 5.2.1 Apriori算法
**性质**：经典频繁项集挖掘算法，基于先验性质剪枝

**先验性质**：
1. 任何非频繁项集的超集都是不频繁的
2. 频繁项集的所有非空子集也一定是频繁的

**算法详细步骤**：
**输入**：事务数据库D，最小支持度阈值min_sup

**输出**：D中的频繁项集L

**算法流程**：
```
1. L1 = find_frequent_1-itemsets(D)
2. for (k=2; Lk-1 ≠ ∅; k++) {
3.   Ck = apriori_gen(Lk-1)  // 候选集生成
4.   for each transaction t in D {
5.     Ct = subset(Ck, t)  // 事务t中包含的候选集
6.     for each candidate c in Ct {
7.       c.count++
8.     }
9.   }
10.  Lk = {c ∈ Ck | c.count ≥ min_sup}
11. }
12. return L = ∪k Lk
```

**候选集生成(apriori_gen)**：
**步骤**：
1. **连接步骤**：
   - 将Lk-1与自身连接
   - 条件：前k-2项相同

2. **剪枝步骤**：
   - 删除(k-1)-子集不在Lk-1中的候选
   - 基于先验性质减少候选数量

**算法优化技术**：
1. **基于散列的技术**：
   - 将候选集散列到不同桶中
   - 直接删除不满足最小支持度的桶

2. **事务压缩**：
   - 不包含任何k项集的事务不可能包含(k+1)项集
   - 删除这些事务减少扫描数据量

3. **划分**：
   - 将数据库分成不重叠区域
   - 先找局部频繁项，再确定全局频繁项

#### 5.2.2 FP-Growth算法
**性质**：无需候选生成的频繁模式挖掘算法，使用特殊数据结构

**核心数据结构**：
- **FP-Tree**：压缩的频繁模式树
- **项头表**：按支持度排序的项列表

**算法详细步骤**：
**输入**：事务数据库D，最小支持度阈值min_sup

**输出**：频繁模式集

**算法流程**：
```
1. 扫描D，计算各项支持度，按降序排列
2. 构建FP-Tree：
3.   创建根节点，标记为null
4.   for each transaction t in D {
5.     按支持度降序排列t中项
6.     insert_tree(sorted_t, root)
7.   }
8. for each item i in项头表(按支持度升序) {
9.   生成i的条件模式基
10.  构建i的条件FP-Tree
11.  if (条件FP-Tree只有单路径) {
12.    生成所有组合
13.  } else {
14.    递归调用FP-Growth
15.  }
16. }
```

**条件模式基生成**：
**步骤**：
1. 从项头表找到项i的所有出现位置
2. 对于每个位置，收集从根到i的路径
3. 记录每条路径的支持度(等于i的支持度)
4. 合并相同路径，累加支持度

#### 5.2.3 Eclat算法
- **要点**：使用垂直数据格式挖掘频繁项集
- **算法过程**：
  1. 通过扫描一次数据集，把水平格式的数据转换成垂直格式
  2. 项集的支持度计数等于项集的TID集的长度
  3. 从k=1开始，使用频繁k项集构造候选(k+1)项集
  4. 通过取频繁k项集的TID集的交，计算对应的(k+1)项集的TID集

### 5.3 频繁模式压缩

#### 5.3.1 闭频繁项集
- **定义**：如果不存在X的真超项集Y使得Y与X在D中具有相同的支持度计数，则称X在D中是closed的
- **特点**：挖掘闭频繁项集可显著减少频繁模式挖掘产生的模式数量，保持完整信息

#### 5.3.2 极大频繁项集
- **定义**：如果不存在频繁项集X的超项集Y，使得Y在D中是频繁的，则称X是D中的极大频繁项集

### 5.4 关联规则评价

#### 5.4.1 提升度和兴趣度
- **提升度**：$Lift(A \Rightarrow B) = \frac{confidence(A \Rightarrow B)}{p(B)} = \frac{P(A \cap B)}{P(A) \times P(B)}$
- **兴趣度**：$Interest(A \Rightarrow B) = \frac{P(A \cap B)}{P(A) \times P(B)} - 1$

#### 5.4.2 KULC度量 + 不平衡比(IR)
- 用于处理不平衡数据中的关联规则评价

### 5.5 多维与多层关联规则

#### 5.5.1 多维关联规则挖掘
- **基于聚类挖掘量化关联规则**：有趣的模式通常在量化属性稠密的簇中发现

#### 5.5.2 多层关联规则挖掘
- **一致支持度**：对所有层都使用一致的最小支持度
- **递减支持度**：在较低层使用递减的最小支持度

### 5.6 近似频繁模式

#### 5.6.1 通过模式聚类挖掘模式
- 距离测量
- δ-聚类：找到距离P的距离小于δ的类$p_i$

#### 5.6.2 提取感知冗余的top-k模式
- **显著性**：$s_{p,q} = s_{p,q} - s_q$
- **冗余性**：$R_{p,q} = S_p + S_q - S(p,q)$

### 5.7 序列模式

#### 5.7.1 基本概念
- **定义**：是一个有序的元素列表，其中每个元素是一个或多个项目的集合
- **子序列**：如果存在整数$1 \leq j_1 < j_2 < \dots < j_m \leq n$使得$t_1 \subseteq s_{j1}, t_2 \subseteq s_{j2}, \dots, t_m \subseteq s_{jm}$，则$t=<t_1 t_2 \dots t_m>$是$s=<s_1 s_2 \dots s_n>$的子序列

#### 5.7.2 序列模式挖掘
- 候选集生成效率问题
- 类Apriori的序列模式挖掘算法

---

## 第六章 聚类

### 6.1 聚类基本概念

#### 6.1.1 什么是聚类
- **定义**：将数据集划分为若干个组(簇)，使得同一组内的数据对象相似度高，不同组间的数据对象相似度低
- **好的聚类方法**：产生高类内相似性、低类间相似性的聚类结果

#### 6.1.2 聚类应用
- 生物学：生物分类
- 信息检索：文档聚类(web搜索)
- 土地利用：识别相似的土地利用区域
- 营销：为对应的客户制定相应的营销计划
- 城市规划：城市功能的科学分类和定量评估
- 地震研究：发掘相似地震
- 气候：理解气候，发现大气和海洋模式
- 图像识别
- 预处理：作为其他算法的预处理步骤
- 离群点检测

#### 6.1.3 聚类分析要素
- **划分准则**：单层vs层次划分
- **簇的分离性**：互斥vs一对多
- **相似性度量**：距离度量vs连接性度量
- **聚类空间**：全空间vs子空间

#### 6.1.4 聚类方法的基本要求
- 可伸缩性
- 对领域知识要求最小化
- 最小化基于知识领域确定的输入参数敏感性
- 可解释性和可用性
- 可发掘任意形状的类别
- 抗噪声
- 增量聚类和对输入次序不敏感
- 聚类高维数据的能力

### 6.2 主要聚类方法

#### 6.2.1 基于代表点的算法

##### K-means算法详解
**性质**：基于划分的聚类算法，最小化类内平方误差

**目标函数**：
$$E = \sum_{i=1}^{k} \sum_{p \in C_i} ||p - c_i||^2$$

**算法详细步骤**：
**输入**：数据集D，聚类数k

**输出**：k个簇

**算法流程**：
```
1. 随机选择k个初始中心点
2. repeat {
3.   // 分配阶段
4.   for each 数据点 p in D {
5.     计算p到各中心点的距离
6.     将p分配到最近中心点所在的簇
7.   }
8.   
9.   // 更新阶段
10.  for each 簇 Ci {
11.    ci = Ci中所有点的均值
12.  }
13. } until 簇不再变化或达到最大迭代次数
```

**算法特点**：
**优点**：
- 简单易实现
- 收敛速度快
- 对球形簇效果好

**缺点**：
- 需要预先指定k值
- 对初始中心点敏感
- 只能发现球形簇
- 对噪声和离群点敏感

**算法变体**：
1. **K-medians**：用中位数代替均值，对异常值不敏感
2. **K-medoids**：用实际数据点作为中心，任意距离度量
3. **模糊C-means**：软聚类，点属于多个簇有隶属度

##### K-medians算法
- 目标函数：$E_1 = \sum_{i=1}^{k} \sum_{p \in C_i} |p - m_i|$
- 最佳代表点是簇每一维度代表点的中值
- 相比K-means，对异常点不那么敏感

##### K-medoids算法
- 从数据集D中选取代表点
- 步骤：
  1. 从D中选择代表点形成代表点集合S的初始值
  2. 用距离函数为每个数据点找到集合S中最近的代表点
  3. 找到D中的数据点以及S中的代表点，使得用代替S中的后目标函数的改进最大
  4. 只有上述改进为正时，才将S中的替换成
  5. 循环直到当前迭代无任何改进

#### 6.2.2 层次聚类算法

##### 凝聚层次聚类
**性质**：自底向上的聚类方法，构建聚类层次结构

**算法流程**：
```
1. 将每个点作为一个簇
2. repeat {
3.   找到最接近的两个簇 Ci, Cj
4.   合并 Ci, Cj 为新簇 Cnew
5.   更新距离矩阵
6. } until 只有一个簇
```

**簇间距离计算方法**：
1. **单链接(MIN)**：
   - $d(C_i, C_j) = \min_{p \in C_i, q \in C_j} d(p,q)$
   - 特点：能处理非球形簇，易受链式效应影响

2. **全链接(MAX)**：
   - $d(C_i, C_j) = \max_{p \in C_i, q \in C_j} d(p,q)$
   - 特点：产生紧凑簇，对异常值敏感

3. **平均链接(AVERAGE)**：
   - $d(C_i, C_j) = \frac{1}{|C_i||C_j|}\sum_{p \in C_i, q \in C_j} d(p,q)$
   - 特点：平衡单链接和全链接

4. **Ward方法**：
   - 目标：最小化合并后的方差增加
   - 公式：$\Delta(i,j) = \frac{|C_i||C_j|}{|C_i|+|C_j|}||c_i-c_j||^2$
   - 特点：倾向于产生大小相近的簇

##### BIRCH算法
- 利用层次结构的平衡迭代规约和聚类
- 阶段1：引入CF树的层次数据结构
- 阶段2：基于叶子结点进行聚类
- 重要参数：枝平衡因子β、叶平衡因子λ、空间阈值τ
- CF(聚类特征)：用三元组(N, LS, SS)概括描述各簇的信息

##### Chameleon算法
- 本质上是一个从下而上的层次聚类算法
- 只考虑每个节点邻近的K个节点
- 使用动态建模确定一对簇之间的相似度
- 步骤：
  1. 使用"图划分算法"将"k临近图"划分为大量相对较小的子簇
  2. 使用"凝聚层次聚类算法"，基于子簇的相似度反复合并子簇

#### 6.2.3 基于密度的算法

##### DBSCAN算法详解
**性质**：基于密度的聚类算法，能发现任意形状簇并处理噪声

**基本概念**：
- **ε-邻域**：点p的ε-邻域$N_\epsilon(p) = \{q | d(p,q) \leq \epsilon\}$
- **核心点**：$|N_\epsilon(p)| \geq MinPts$
- **边界点**：不是核心点但在某个核心点的ε-邻域内
- **噪声点**：既不是核心点也不是边界点

**算法详细步骤**：
**输入**：数据集D，半径ε，最小点数MinPts

**输出**：簇集合

**算法流程**：
```
1. 初始化：所有点标记为unvisited
2. for each 点 p in D {
3.   if (p是visited) continue
4.   标记p为visited
5.   N = ε-邻域(p)
6.   if (|N| < MinPts) {
7.     标记p为噪声
8.   } else {
9.     创建新簇C
10.    expand_cluster(p, N, C)
11.  }
12. }
```

**簇扩展算法**：
```
expand_cluster(p, N, C) {
  将p加入C
  for each 点 q in N {
    if (q是unvisited) {
      标记q为visited
      N' = ε-邻域(q)
      if (|N'| ≥ MinPts) {
        N = N ∪ N'
      }
    }
    if (q不属于任何簇) {
      将q加入C
    }
  }
}
```

**算法特点**：
**优点**：
- 能发现任意形状簇
- 不需要指定簇数量
- 能处理噪声点

**缺点**：
- 对参数ε和MinPts敏感
- 难以处理密度不均的数据
- 高维数据效果差

##### OPTICS算法
- Ordering Points To Identify the Clustering Structure
- 将eps指定为一个范围，而非一个固定值
- 计算一个簇次序，代表数据的基于密度的聚类结构
- 基本概念：
  - 核心距离：使得x成为核心点的最小邻域半径
  - 可达距离：假设p是核心点，点p和q的可达距离定义为$max(core\_distance(p), distance(p,q))$

##### DENCLUE算法
- 引入影响函数和密度函数的概念
- 空间中任一点的密度是所有数据点在此点产生影响的叠加
- 使用密度吸引子将数据划分成簇

#### 6.2.4 基于网格的算法
- **STING**：
  - 统计信息网格方法
  - 用多分辨率的网格结构表达原始数据
  - 将空间区域划分成网格单元
  - 不同层次的网格表示不同分辨率下的数据特征

- **CLIQUE**：
  - 基于密度和网格的算法
  - 把每一维都划分成等长的区间
  - 将m维的数据划分成互不重叠的长方形子区间
  - 如果一个区间中的数据点个数超过阈值，则认为该区间是稠密单元

#### 6.2.5 基于概率模型的算法
- **EM算法**：
  - 软聚类算法
  - 假设数据由数据分布为$\theta_k$的k个分布混合生成
  - 每个分布代表一个簇，也称为混合分量
  - 步骤：
    1. E步骤：将之前未见到的点分配给某一个类别
    2. M步骤：将概率分布的参数进行优化

#### 6.2.6 基于图的方法

##### 谱聚类算法详解
**性质**：基于图论的聚类方法，将聚类转化为图分割问题

**数学基础**：
- **相似度图**：节点表示数据点，边权重表示相似度
- **拉普拉斯矩阵**：$L = D - W$，其中D是度矩阵，W是邻接矩阵
- **谱聚类**：利用拉普拉斯矩阵的特征向量进行聚类

**算法详细步骤**：
**输入**：数据集X，聚类数k

**输出**：k个簇

**算法流程**：
```
1. 构建相似度矩阵W：
2.   for each i, j {
3.     W[i,j] = similarity(xi, xj)
4.   }
5. 构建度矩阵D：D[i,i] = Σj W[i,j]
6. 计算拉普拉斯矩阵L = D - W
7. 计算L的前k个最小特征值对应的特征向量
8. 将特征向量组成n×k矩阵U
9. 对U的行进行K-means聚类
10. 返回聚类结果
```

**相似度度量选择**：
1. **高斯核**：$W[i,j] = \exp(-\frac{||x_i-x_j||^2}{2\sigma^2})$
2. **k近邻**：只保留k个最近邻的连接
3. **全连接**：所有点对都有连接

### 6.3 聚类评估

#### 6.3.1 数据的聚集程度评估
- **熵**：可以反馈特征子集的聚类质量
- **霍普金斯统计**：
  - $H = \frac{\sum_{i=1}^{m}u_i}{\sum_{i=1}^{m}u_i + \sum_{i=1}^{m}w_i}$
  - 均匀分布时，H值为0.5；聚集时，H值大于0.5

#### 6.3.2 K值确定
- **经验法**：
- **"肘"方法**：对n个点的数据集，迭代计算k from 1 to n，计算每个点到其所属簇中心的距离的平方和
- **交叉验证**

#### 6.3.3 聚类质量
- **内部验证度量**：
  - **Silhouette系数**：
    - $a(i)$：对象i与同一簇中其他对象的平均距离
    - $b(i)$：对象i与不同簇中对象的平均距离的最小值
    - $s(i) = \frac{b(i)-a(i)}{max\{a(i),b(i)\}}$
    - Silhouette系数接近1表示聚类效果好

- **外部验证度量**：
  - **纯度**：$\frac{1}{N}\sum_k max_j |cluster_k \cap class_j|$
  - **基尼系数**：$1 - \sum_j (\frac{|cluster_k \cap class_j|}{|cluster_k|})^2$

---

## 第七章 离群点检测

### 7.1 基本概念

#### 7.1.1 离群点定义
- **Hawkins定义**：离群点是在数据集中偏离大部分数据的数据，使人怀疑这些数据的偏离并非由随机因素产生，而是完全产生于不同的机制
- **Weisberg定义**：离群点是与数据集中其余部分不服从相同统计模型的数据
- **Samuels定义**：离群点是数据集中足够地不同于数据集中其余部分的数据
- **Porkess定义**：离群点是远离数据集中其余部分的数据

#### 7.1.2 噪声vs离群点
- **噪声**：是一个测量变量中的随机错误或偏差，包括错误的值，偏离期望的孤立点

#### 7.1.3 离群点类型
- **全局离群点**
- **局部离群点**：情境(条件)离群点
- **集体离群点**：数据对象的子集形成集体离群点

#### 7.1.4 应用
- 欺诈检测
- 灾害预报
- 嫌疑人判定
- 群体检测(疫情)
- 罕见病检测

### 7.2 离群点检测方法

#### 7.2.1 基于密度的方法
- **直方图**：假设每个维度间都是独立的，分别计算一个样本在不同维度上所处的密度空间，并叠加结果
- **核密度估计**：
  - $f(\vec{x}) = \frac{1}{n}\sum_{i=1}^{n}K(\vec{x} - \vec{x}_i)$
  - $K(\vec{x} - \vec{x}_i) = \frac{1}{\sigma\sqrt{2\pi}^d}e^{-\frac{||\vec{x} - \vec{x}_i||^2}{2\sigma^2}}$

#### 7.2.2 基于概率的方法
- **基本思想**：识别模型低概率区域中的对象
- **一元离群点检测**：
  - 密度分布函数$f(x;x;\theta)$
  - $f(x;x;\theta) \leq \theta$
  
- **多元离群点检测**：
  - $f(\vec{x}) = \frac{1}{|\Sigma| \cdot (2 \cdot \pi)^{d/2}} \cdot e^{-\frac{1}{2}(\vec{x} - \vec{\mu})\Sigma^{-1}(\vec{x} - \vec{\mu})^T}$
  - $Mahalanobis(\vec{x}, \vec{\mu}, \Sigma)^2$

- **使用混合参数分布**：
  - 构建似然函数
  - 通过迭代的EM算法求解模型参数和概率分布

#### 7.2.3 基于距离的方法
- **基本思路**：数据集中显著偏离其它对象的点是离群点
- **判定依据**：
  - A. 给定邻域半径，o的r邻域所包含的对象个数占比
  - B. 依据o与其第k个邻近之间的距离(或平均距离)判定o是否离群

- **基于距离的离群点检测算法**：
  - 输入：对象集D，阈值r和π
  - 输出：D中的DB(r,π)离群点

- **局部距离的修正**：
  - 从o'到o的可达距离：$reach\_dist_k(o,o') = max\{k\_distance(o), distance(o,o')\}$
  - o的局部可达密度：$lrd_k(o) = \frac{1}{\sum_{o' \in N_k(o)} \frac{reach\_dist_k(o,o')}{|N_k(o)|}}$
  - 对象o的局部离群点因子LOF：$LOF_k(o) = \frac{\sum_{o' \in N_k(o)} \frac{lrd_k(o')}{lrd_k(o)}}{|N_k(o)|}$

#### 7.2.4 基于聚类的方法
- **基本思想**：
  - 建立正常模型
  - 离群点为不能正常地符合这个模型的数据点
  - 将异常程度量化为数值(异常分)
  
- **形式**：
  - 基于聚类产生簇，寻找远离簇的数据点
  - 考虑对象和它最近簇之间的距离
  - 不属于任何Cluster或远离任何Cluster

- **FindCBLOF**：检测小簇中的离群点
  - 聚类，将类别降序排序
  - 赋予每个点一个基于簇的离群点检测因子(CBLOF)

#### 7.2.5 基于分类的方法
- **基本思想**：训练一个可以区分正常数据离群点的分类模型
- **One-Class模型**：
  - 与二分类和多分类的区别
  - 将不属于该类的所有其他样本判别为"不是"
  
- **One-Class SVM——SVDD**：
  - 采用一个超球体而不是一个超平面来做划分
  - 在特征空间中获得数据周围的球形边界
  - 期望最小化这个超球体的体积

- **半监督学习**：结合聚类和分类检测离群点

#### 7.2.6 其它方法
- **Isolation Forest**：
  - 思路：异常样本相较普通样本可以通过较少次数的随机特征分割被孤立出来
  - 利用决策树的特点，多次对特征空间进行划分，观察"孤立"一个点难易程度
  - 异常点往往在决策树的深度比较低的叶子结点

- **自编码器(Autoencoder)**：
  - 思路：通过自编码器的先压缩(encoder)再还原(decoder)，异常点会有更大的重建误差

### 7.3 模型集成与评价

#### 7.3.1 多模型合并
- **直接合并**：缺点是结果平庸
- **动态模型选择**：
  - 对于每个测试点生成局部空间，也即近邻
  - 模型选择与合并

#### 7.3.2 模型评价
- **外部度量**：将从人工合成数据集或真实数据集的罕见类别中获得的已知异常点标签作为真值
- **ROC曲线**：处理假正例和假负例平衡

---

## 挖掘Web数据：PageRank算法

### 8.1 Web作为有向图
- Web页面可以看作有向图中的节点
- 超链接可以看作有向边

### 8.2 PageRank基本思想
- **链接作为投票**：
  - 页面更重要如果它有更多链接
  - 入链作为投票
  - 来自重要页面的链接计票更多
  - 递归问题

### 8.3 PageRank数学模型

#### 8.3.1 流动模型
- 每个链接的投票与源页面重要性成正比
- 如果页面j有重要性$r_j$和$n$个出链，每个链接得到$r_j/n$票
- 页面j的重要性是入链投票之和：$r_j = \sum_{i \to j} \frac{r_i}{d_i}$

#### 8.3.2 矩阵形式
- **随机邻接矩阵M**：
  - 如果页面i有$d_i$个出链
  - 如果$i \to j$，则$M_{ji} = \frac{1}{d_i}$，否则$M_{ji} = 0$
  - M是列随机矩阵，列和为1

- **秩向量r**：每个页面对应一个重要性得分
- 流动方程：$r = M \cdot r$

#### 8.3.3 特征向量形式
- 秩向量r是随机web矩阵M的特征向量
- 对应特征值为1的主特征向量

### 8.4 PageRank计算方法

#### 8.4.1 幂迭代方法
- 初始化：$r^{(0)} = [1/N,....,1/N]^T$
- 迭代：$r^{(t+1)} = M \cdot r^{(t)}$
- 停止条件：$|r^{(t+1)} - r^{(t)}|_1 < \epsilon$

#### 8.4.2 问题与解决方案
- **问题1：死端(Dead ends)**：没有出链的页面导致重要性"泄漏"
- **问题2：蜘蛛陷阱(Spider traps)**：所有出链在组内，随机游走被"困住"

- **解决方案：随机传送**：
  - 在每一步，随机冲浪者有两个选择：
    - 以概率β，随机跟随一个链接
    - 以概率1-β，跳转到某个随机页面
  - 常见β值范围：0.8到0.9

#### 8.4.3 Google矩阵
- **PageRank方程**：$r_j = \sum_{i \to j} \frac{\beta r_i}{d_i} + \frac{1-\beta}{N}$
- **Google矩阵A**：$A = \beta M + \frac{1-\beta}{N}N \times N$
- 递归问题：$r = A \cdot r$

### 8.5 主题特定PageRank
- **目标**：评估Web页面不仅根据通用流行度，还根据与特定主题的接近程度
- **方法**：
  - 随机游走者有很小概率在任何步骤传送
  - 传送到主题相关的"相关"页面集合(传送集)
  - 偏向随机游走
  - 当游走者传送时，从集合S中选择一个页面
  - 对于每个传送集S，得到不同的向量$r_S$

- **矩阵形式**：
  - 更新传送部分的PageRank公式：
    - $A_{ij} = \beta M_{ij} + \frac{1-\beta e_s}{|S|}$ 如果$i \in S$
    - $A_{ij} = \beta M_{ij} + 0$ 否则

---

## 算法复杂度分析

### 时间复杂度总结
| 算法 | 时间复杂度 | 空间复杂度 | 适用场景 |
|------|------------|------------|----------|
| Apriori | O(k×n×m) | O(n×m) | 小数据集，低维 |
| FP-Growth | O(n×m) | O(n×m) | 大数据集，密集数据 |
| K-means | O(t×k×n×d) | O(k×d) | 球形簇，大数据 |
| 层次聚类 | O(n²) | O(n²) | 小数据集，层次结构 |
| DBSCAN | O(n×log n) | O(n) | 任意形状，空间数据 |
| 谱聚类 | O(n³) | O(n²) | 中小数据集，图结构 |
| LOF | O(n×log n) | O(n) | 局部离群点检测 |
| PageRank | O(k×n) | O(n²) | Web图，链接分析 |

*注：n-样本数，m-项数，k-簇数/迭代次数，d-维度，t-迭代次数*

---

## 考试复习要点

### 1. 计算题重点
- 数据预处理中的标准化计算
- 距离和相似性度量计算
- 关联规则支持度和置信度计算
- Apriori算法候选集生成和剪枝
- K-means聚类过程
- PageRank计算

### 2. 简答题重点
- 各章节基本概念和定义
- 各种算法的基本思想和流程
- 算法间的比较和优缺点
- 方法适用场景

### 3. 论述分析题重点
- 针对具体案例应用大数据挖掘思路进行完整阐述
- 从方法到评价的全面分析
- 经典方法的原理、运行流程、方法间根本不同和关键改进

### 4. 特别注意事项
- 闭频繁和极大频繁需要掌握
- 近似频繁模式了解即可
- PCA需要理解原理和用途
- 推荐算法章节不在考试范围
- PPT上的例题要能换数据计算

---

*本文档根据课程PPT内容和老师提示消息整理，涵盖了"大数据挖掘与分析"课程的主要知识点，重点突出了考试相关内容。*